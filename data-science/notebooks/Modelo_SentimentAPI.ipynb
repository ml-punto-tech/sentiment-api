{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtJ3Iy0IKFDG"
      },
      "source": [
        "# <font size=35 color=lightgreen>** Sentiment API **<font>ü•≤\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1WimRtik1c6"
      },
      "source": [
        "### <font size=12 color=lightgreen>Configuraci√≥n Inicial (Librer√≠as)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv3A7xMRl-DP"
      },
      "source": [
        "#### 1. Procesamiento y Manipulaci√≥n de Datos\n",
        "* **`pandas`**\n",
        "    * Nos ayuda con la manipulaci√≥n y an√°lisis de datos estructurados.\n",
        "    * Carga el dataset (CSV), gestiona el DataFrame y permite filtrar o limpiar registros.\n",
        "* **`numpy`**\n",
        "    * Realiza las operaciones matem√°ticas y manejo de arrays eficientes.\n",
        "    * Soporte num√©rico fundamental para las transformaciones vectoriales de los textos.\n",
        "\n",
        "#### 2. Visualizaci√≥n y An√°lisis Exploratorio\n",
        "\n",
        "* **`matplotlib.pyplot`**\n",
        "    * Generaci√≥n de gr√°ficos est√°ticos.\n",
        "    * Visualizaci√≥n b√°sica de la distribuci√≥n de clases (Positivo vs. Negativo).\n",
        "* **`seaborn`**\n",
        "    * Visualizaci√≥n de datos estad√≠sticos avanzada.\n",
        "    * Generaci√≥n de matrices de confusi√≥n y gr√°ficos de distribuci√≥n est√©ticos para la presentaci√≥n.\n",
        "\n",
        "#### 3. Procesamiento de Lenguaje Natural (NLP) y Limpieza\n",
        "\n",
        "* **`re`** (Regular Expressions)\n",
        "    * Manejo de expresiones regulares.\n",
        "    * Eliminaci√≥n de ruido en el texto: URLs, menciones (@usuario), hashtags (#) y caracteres especiales no alfanum√©ricos.\n",
        "* **`string`**\n",
        "    * Constantes de cadenas comunes.\n",
        "    * Provee listas est√°ndar de signos de puntuaci√≥n para su eliminaci√≥n eficiente.\n",
        "\n",
        "#### 4. Modelado y Machine Learning (Core)\n",
        "\n",
        "* **`scikit-learn`**\n",
        "    * Biblioteca principal de Machine Learning.\n",
        "    * **`TfidfVectorizer`**: Transforma el texto limpio en vectores num√©ricos.\n",
        "    * **`LogisticRegression`**: Algoritmo de clasificaci√≥n supervisada.\n",
        "    * **`metrics`**: C√°lculo de precisi√≥n, recall y F1-score.\n",
        "    * **`Pipeline`**: Encapsulamiento de los pasos de transformaci√≥n y predicci√≥n.\n",
        "\n",
        "#### 5. Persistencia e Integraci√≥n\n",
        "Herramientas para conectar el modelo con el Backend.\n",
        "\n",
        "* **`joblib`**\n",
        "    * Serializaci√≥n eficiente de objetos Python.\n",
        "    * Exportar (`dump`) el pipeline entrenado a un archivo `.joblib` y cargarlo (`load`) en la API para realizar predicciones.\n",
        "* **`fastapi` & `uvicorn`**\n",
        "    * Framework web moderno de alto rendimiento.\n",
        "    * Exponer el modelo entrenado como un microservicio REST (endpoint `/predict`) para ser consumido por el Backend en Java.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tELAqUZeOA7W"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VengB6XbODtf"
      },
      "source": [
        "### <font size=16  color=lightgreen> Importando librer√≠as <font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1295,
=======
      "execution_count": 1271,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
      "metadata": {
        "id": "0LqeO8Iig4ZI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import chardet\n",
        "import uvicorn\n",
        "import sklearn\n",
        "import fastapi\n",
        "import joblib\n",
        "import nltk\n",
        "import unicodedata\n",
        "import urllib.request\n",
        "from io import StringIO\n",
        "import urllib.response\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font size = 8 color=\"lightgreen\">Importaci√≥n de diccionario<font>"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1296,
=======
      "execution_count": 1272,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import urllib.request\n",
        "from urllib.error import URLError, HTTPError\n",
        "\n",
        "def cargar_diccionario_sentimientos(url):\n",
        "    \"\"\"\n",
        "    Carga diccionario de sentimientos desde URL raw de GitHub.\n",
        "    \n",
        "    Args:\n",
        "        url: URL del archivo JSON en GitHub (debe ser raw)\n",
        "    \n",
        "    Returns:\n",
        "        Diccionario con la clasificaci√≥n de sentimientos\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Verificar que sea URL raw\n",
        "        if \"github.com\" in url and \"raw.githubusercontent.com\" not in url:\n",
        "            # Convertir URL normal a raw\n",
        "            url = url.replace(\"github.com\", \"raw.githubusercontent.com\")\n",
        "            url = url.replace(\"/blob/\", \"/\")\n",
        "            print(f\"üîÑ URL convertida a raw: {url}\")\n",
        "        \n",
        "        # Descargar el archivo\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            content = response.read()\n",
        "        \n",
        "        # Decodificar y cargar JSON\n",
        "        datos = json.loads(content.decode('utf-8'))\n",
        "        \n",
        "        print(f\"‚úÖ Diccionario cargado: {len(datos)} sentimientos\")\n",
        "        \n",
        "        # Crear listas de categor√≠as\n",
        "        positivos = [k for k, v in datos.items() if v == 'positivo']\n",
        "        negativos = [k for k, v in datos.items() if v == 'negativo']\n",
        "        neutros = [k for k, v in datos.items() if v == 'neutral']\n",
        "        \n",
        "        print(f\"üìä Positivos: {len(positivos)}, Negativos: {len(negativos)}, Neutros: {len(neutros)}\")\n",
        "        \n",
        "        return {\n",
        "            'datos': datos,\n",
        "            'positivos': positivos,\n",
        "            'negativos': negativos,\n",
        "            'neutros': neutros\n",
        "        }\n",
        "        \n",
        "    except HTTPError as e:\n",
        "        print(f\"‚ùå Error HTTP {e.code}: {e.reason}\")\n",
        "        print(f\"üí° Verifica que la URL sea correcta y el archivo exista\")\n",
        "    except URLError as e:\n",
        "        print(f\"‚ùå Error de URL: {e.reason}\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Error al decodificar JSON: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error inesperado: {type(e).__name__}: {e}\")\n",
        "    \n",
        "    # Retornar diccionario vac√≠o en caso de error\n",
        "    return {\n",
        "        'datos': {},\n",
        "        'positivos': [],\n",
        "        'negativos': [],\n",
        "        'neutros': []\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1297,
=======
      "execution_count": 1273,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ URL convertida a raw: https://raw.githubusercontent.com/ml-punto-tech/sentiment-api/dev/data-science/sources/diccionarios/sentimientos_mapeo.json\n",
            "‚úÖ Diccionario cargado: 106 sentimientos\n",
            "üìä Positivos: 62, Negativos: 39, Neutros: 5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'datos': {'abrumado': 'negativo',\n",
              "  'aburrimiento': 'negativo',\n",
              "  'aceptacion': 'positivo',\n",
              "  'admiracion': 'positivo',\n",
              "  'adoracion': 'positivo',\n",
              "  'agradecido': 'positivo',\n",
              "  'aislamiento': 'negativo',\n",
              "  'alegria': 'positivo',\n",
              "  'amabilidad': 'positivo',\n",
              "  'amargura': 'negativo',\n",
              "  'ambivalencia': 'neutral',\n",
              "  'amistad': 'positivo',\n",
              "  'amor': 'positivo',\n",
              "  'angustia': 'negativo',\n",
              "  'anhelo': 'negativo',\n",
              "  'ansiedad': 'negativo',\n",
              "  'anticipacion': 'neutral',\n",
              "  'apreciacion': 'positivo',\n",
              "  'aprensivo': 'negativo',\n",
              "  'armonia': 'positivo',\n",
              "  'arrepentimiento': 'negativo',\n",
              "  'asco': 'negativo',\n",
              "  'asombro': 'positivo',\n",
              "  'cautivacion': 'positivo',\n",
              "  'celebracion': 'positivo',\n",
              "  'colorido': 'positivo',\n",
              "  'confiado': 'positivo',\n",
              "  'confianza': 'positivo',\n",
              "  'contentamiento': 'positivo',\n",
              "  'creatividad': 'positivo',\n",
              "  'cumplimiento': 'positivo',\n",
              "  'curiosidad': 'neutral',\n",
              "  'decepcion': 'negativo',\n",
              "  'desamor': 'negativo',\n",
              "  'descubrimiento': 'positivo',\n",
              "  'desesperacion': 'negativo',\n",
              "  'deslumbrar': 'positivo',\n",
              "  'despectivo': 'negativo',\n",
              "  'determinacion': 'positivo',\n",
              "  'devastado': 'negativo',\n",
              "  'disfrute': 'positivo',\n",
              "  'diversion': 'positivo',\n",
              "  'dolor': 'negativo',\n",
              "  'elacion': 'positivo',\n",
              "  'elegancia': 'positivo',\n",
              "  'emocion': 'positivo',\n",
              "  'empoderamiento': 'positivo',\n",
              "  'empatico': 'positivo',\n",
              "  'encantamiento': 'positivo',\n",
              "  'energia': 'positivo',\n",
              "  'enojo': 'negativo',\n",
              "  'entumecimiento': 'negativo',\n",
              "  'entusiasmo': 'positivo',\n",
              "  'envidia': 'negativo',\n",
              "  'envidioso': 'negativo',\n",
              "  'esperanza': 'positivo',\n",
              "  'euforia': 'positivo',\n",
              "  'excitacion': 'positivo',\n",
              "  'felicidad': 'positivo',\n",
              "  'frustracion': 'negativo',\n",
              "  'frustrado': 'negativo',\n",
              "  'grandeza': 'positivo',\n",
              "  'gratitud': 'positivo',\n",
              "  'inspiracion': 'positivo',\n",
              "  'inspirado': 'positivo',\n",
              "  'intimidacion': 'positivo',\n",
              "  'jugueton': 'positivo',\n",
              "  'logro': 'positivo',\n",
              "  'lastima': 'negativo',\n",
              "  'malo': 'negativo',\n",
              "  'maravilla': 'positivo',\n",
              "  'melancolia': 'negativo',\n",
              "  'melodico': 'positivo',\n",
              "  'miedo': 'negativo',\n",
              "  'motivacion': 'positivo',\n",
              "  'negativo': 'negativo',\n",
              "  'neutral': 'neutral',\n",
              "  'obstaculo': 'negativo',\n",
              "  'odiar': 'negativo',\n",
              "  'optimismo': 'positivo',\n",
              "  'orgullo': 'positivo',\n",
              "  'pena': 'negativo',\n",
              "  'positividad': 'positivo',\n",
              "  'positivo': 'positivo',\n",
              "  'perdida': 'negativo',\n",
              "  'reconfortante': 'positivo',\n",
              "  'reflexion': 'negativo',\n",
              "  'resentimiento': 'negativo',\n",
              "  'resiliencia': 'positivo',\n",
              "  'resplandor': 'positivo',\n",
              "  'reverencia': 'positivo',\n",
              "  'romance': 'positivo',\n",
              "  'satisfaccion': 'positivo',\n",
              "  'serenidad': 'positivo',\n",
              "  'soledad': 'negativo',\n",
              "  'sorpresa': 'neutral',\n",
              "  'sufrimiento': 'negativo',\n",
              "  'temeroso': 'negativo',\n",
              "  'ternura': 'positivo',\n",
              "  'traicion': 'negativo',\n",
              "  'tristeza': 'negativo',\n",
              "  'triunfo': 'positivo',\n",
              "  'verguenza': 'negativo',\n",
              "  '√°nimo': 'positivo',\n",
              "  'exito': 'positivo',\n",
              "  'extasis': 'positivo'},\n",
              " 'positivos': ['aceptacion',\n",
              "  'admiracion',\n",
              "  'adoracion',\n",
              "  'agradecido',\n",
              "  'alegria',\n",
              "  'amabilidad',\n",
              "  'amistad',\n",
              "  'amor',\n",
              "  'apreciacion',\n",
              "  'armonia',\n",
              "  'asombro',\n",
              "  'cautivacion',\n",
              "  'celebracion',\n",
              "  'colorido',\n",
              "  'confiado',\n",
              "  'confianza',\n",
              "  'contentamiento',\n",
              "  'creatividad',\n",
              "  'cumplimiento',\n",
              "  'descubrimiento',\n",
              "  'deslumbrar',\n",
              "  'determinacion',\n",
              "  'disfrute',\n",
              "  'diversion',\n",
              "  'elacion',\n",
              "  'elegancia',\n",
              "  'emocion',\n",
              "  'empoderamiento',\n",
              "  'empatico',\n",
              "  'encantamiento',\n",
              "  'energia',\n",
              "  'entusiasmo',\n",
              "  'esperanza',\n",
              "  'euforia',\n",
              "  'excitacion',\n",
              "  'felicidad',\n",
              "  'grandeza',\n",
              "  'gratitud',\n",
              "  'inspiracion',\n",
              "  'inspirado',\n",
              "  'intimidacion',\n",
              "  'jugueton',\n",
              "  'logro',\n",
              "  'maravilla',\n",
              "  'melodico',\n",
              "  'motivacion',\n",
              "  'optimismo',\n",
              "  'orgullo',\n",
              "  'positividad',\n",
              "  'positivo',\n",
              "  'reconfortante',\n",
              "  'resiliencia',\n",
              "  'resplandor',\n",
              "  'reverencia',\n",
              "  'romance',\n",
              "  'satisfaccion',\n",
              "  'serenidad',\n",
              "  'ternura',\n",
              "  'triunfo',\n",
              "  '√°nimo',\n",
              "  'exito',\n",
              "  'extasis'],\n",
              " 'negativos': ['abrumado',\n",
              "  'aburrimiento',\n",
              "  'aislamiento',\n",
              "  'amargura',\n",
              "  'angustia',\n",
              "  'anhelo',\n",
              "  'ansiedad',\n",
              "  'aprensivo',\n",
              "  'arrepentimiento',\n",
              "  'asco',\n",
              "  'decepcion',\n",
              "  'desamor',\n",
              "  'desesperacion',\n",
              "  'despectivo',\n",
              "  'devastado',\n",
              "  'dolor',\n",
              "  'enojo',\n",
              "  'entumecimiento',\n",
              "  'envidia',\n",
              "  'envidioso',\n",
              "  'frustracion',\n",
              "  'frustrado',\n",
              "  'lastima',\n",
              "  'malo',\n",
              "  'melancolia',\n",
              "  'miedo',\n",
              "  'negativo',\n",
              "  'obstaculo',\n",
              "  'odiar',\n",
              "  'pena',\n",
              "  'perdida',\n",
              "  'reflexion',\n",
              "  'resentimiento',\n",
              "  'soledad',\n",
              "  'sufrimiento',\n",
              "  'temeroso',\n",
              "  'traicion',\n",
              "  'tristeza',\n",
              "  'verguenza'],\n",
              " 'neutros': ['ambivalencia',\n",
              "  'anticipacion',\n",
              "  'curiosidad',\n",
              "  'neutral',\n",
              "  'sorpresa']}"
            ]
          },
<<<<<<< HEAD
          "execution_count": 1297,
=======
          "execution_count": 1273,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = cargar_diccionario_sentimientos('https://github.com/ml-punto-tech/sentiment-api/blob/dev/data-science/sources/diccionarios/sentimientos_mapeo.json')\n",
        "url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEXpMdxbOQAV"
      },
      "source": [
        "### <font size = 8 color=\"lightgreen\">Importaci√≥n de datasets<font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHpgAk4eZxyY"
      },
      "source": [
        "#### **Funci√≥n importaci√≥n dataset**"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1298,
=======
      "execution_count": 1274,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
      "metadata": {
        "id": "yOwHw3xtYJEg"
      },
      "outputs": [],
      "source": [
        "def importar_dataset(url, sep=';'):\n",
        "    \"\"\"\n",
        "    Importa dataset desde URL detectando encoding autom√°ticamente.\n",
        "    Acepta un argumento `sep` para especificar el separador del CSV (por defecto ';').\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Descargar contenido una sola vez\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            content = response.read()\n",
        "\n",
        "        # 2. Detectar encoding\n",
        "        result = chardet.detect(content)\n",
        "        encoding = result['encoding']\n",
        "        print(f\"üîç Encoding detectado: {encoding} (confianza: {result['confidence']:.2%})\")\n",
        "\n",
        "        # 3. Decodificar y cargar en DataFrame\n",
        "        decoded_content = content.decode(encoding, errors='replace')\n",
        "        data = pd.read_csv(StringIO(decoded_content), sep=sep)\n",
        "\n",
        "        print(\"‚úÖ Archivo cargado correctamente\")\n",
        "        print(f\"üìä Tama√±o del dataset: {data.shape}\")\n",
        "        print(\"\\nüîç Muestra aleatoria (3 registros):\")\n",
        "        print(data.sample(3))\n",
        "\n",
        "        return data\n",
        "\n",
        "    except urllib.error.URLError as e:\n",
        "        print(f\"‚ùå Error de URL: {e}\")\n",
        "        return None\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(f\"‚ùå Error al parsear CSV: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error inesperado: {type(e).__name__}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDZW5B7jk5-x"
      },
      "source": [
        "#### **dataset1_es.csv**"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1299,
=======
      "execution_count": 1275,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWgPb0VhYeKT",
        "outputId": "1265d274-21aa-4f74-b3b7-e0a7d18e6a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Encoding detectado: Windows-1254 (confianza: 81.50%)\n",
            "‚úÖ Archivo cargado correctamente\n",
            "üìä Tama√±o del dataset: (1466, 1)\n",
            "\n",
            "üîç Muestra aleatoria (3 registros):\n",
<<<<<<< HEAD
            "                                                                                                                                                                                                 <<<<<<< HEAD:data-science/datasets/datasets-origin/sentimentdataset_es.csv\n",
            "110 112 Revisando viejos recuerdos, sintiendo una sensa... Positivo 15-05-2010 15:30 NostalgiaFan                Twitter   #Euforia #Recuerdos                            20 40 Canad√° 2010 5 15                                                 15                        \n",
            "677 681 Elaboraci√≥n de intrincadas pulseras de la amist... Alegr√≠a  04-09-2023 15:15 BraceletCraftsmanHighSchool Instagram #Pulserasdelaamistad #ManualidadesdeSecundaria 19 37 EE.UU  2023 9 4                                                  15                        \n",
            "303 307 Abrumado por el peso del mundo, Atlas con los h... Abrumado 25-06-2021 17:50 AtlasBearer                 Instagram #Abrumado #BouldersOfExhaustion                22 44 Canad√° 2021 6 25                                                 17                        \n"
=======
            "                                                                                                                                                                                                   <<<<<<< HEAD:data-science/datasets/datasets-origin/sentimentdataset_es.csv\n",
            "673 677 Emb√°rcate en una misi√≥n para encontrar la mejor... Excitaci√≥n 31-08-2023 20:15 BurgerQuestHighSchool Twitter  #BurgerQuest #Amigos gastron√≥micos de secundaria 24 42 Reino Unido 2023 8 31                                                 20                        \n",
            "94  96  Amarga experiencia en el departamento de atenci... Negativo   26-02-2023 15:00 CustomerWoes          Facebook #ExperienciaAmarga #ServicioAlCliente            18 35 EE.UU       2023 2 26                                                 15                        \n",
            "191 193 El dolor abruma, una tormenta de emociones en s... Dolor      25-02-2021 15:30 StormyHeart           Twitter  #Duelo #TormentaEmocional                        12 25 Canad√°      2021 2 25                                                 15                        \n"
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
          ]
        }
      ],
      "source": [
        "df1_url_es = importar_dataset(\"https://github.com/ml-punto-tech/sentiment-api/raw/refs/heads/dev/data-science/datasets/datasets-origin/dataset1_esp.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKr3W1NEaBrP"
      },
      "source": [
        "#### **dataset2_es.csv**"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1300,
=======
      "execution_count": 1276,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2eRhM-MYh6L",
        "outputId": "e224480a-7b3e-44c6-ec3f-dea83217a6af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Encoding detectado: Windows-1252 (confianza: 73.00%)\n",
            "‚úÖ Archivo cargado correctamente\n",
            "üìä Tama√±o del dataset: (2540, 3)\n",
            "\n",
            "üîç Muestra aleatoria (3 registros):\n",
            "                                                  texto  label sentimiento\n",
<<<<<<< HEAD
            "1657  \"Abrazo sincero y c√°lido. Un lugar inexplicabl...      2    positivo\n",
            "734   Dimos a un CP3 retirado y lo peor de todo es q...      0    negativo\n",
            "1664  que rabia que sea tan hermoso, seguro todas le...      2    positivo\n"
=======
            "75    Cartagena y su gobierno local est√°n preocupado...      0    negativo\n",
            "2332  Mucha paz en mi vida. Aunque llegar a este pun...      2    positivo\n",
            "1071  Si me duermo a las 9 me levanto a las 3 fresco...      0    negativo\n"
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
          ]
        }
      ],
      "source": [
        "df2_url_es = importar_dataset(\"https://github.com/ml-punto-tech/sentiment-api/raw/refs/heads/dev/data-science/datasets/datasets-origin/dataset2_esp.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **dataset3_es.csv**"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1301,
=======
      "execution_count": 1277,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmMcmf9gyZpS",
        "outputId": "59797edf-93fb-42fc-b82e-8f20bf2f8763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Encoding detectado: Windows-1254 (confianza: 56.84%)\n",
            "‚úÖ Archivo cargado correctamente\n",
            "üìä Tama√±o del dataset: (740, 4)\n",
            "\n",
            "üîç Muestra aleatoria (3 registros):\n",
            "       id   plataforma sentimiento  \\\n",
<<<<<<< HEAD
            "450  2716  Borderlands     Neutral   \n",
            "429  9004       Nvidia     Neutral   \n",
            "21   9035       Nvidia     Neutral   \n",
            "\n",
            "                                                 texto  \n",
            "450  Este ser√≠a un casting incre√≠ble, y sin embargo...  \n",
            "429  Mi buen amigo @MrBadBit y yo hicimos este par ...  \n",
            "21                             Mira la parte 4 de Para  \n"
=======
            "148  9115       Nvidia     Neutral   \n",
            "715  8032    Microsoft     Neutral   \n",
            "410  2698  Borderlands     Neutral   \n",
            "\n",
            "                                                 texto  \n",
            "148  @MicrosoftHelpt @TreyarchPC @ATVIAssist Tengo ...  \n",
            "715  Al imponer el uso de Bing en los usuarios de C...  \n",
            "410  Borderlands 3: ¬øVali√≥ la pena el pase de tempo...  \n"
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
          ]
        }
      ],
      "source": [
        "df3_url_es = importar_dataset(\"https://github.com/ml-punto-tech/sentiment-api/raw/refs/heads/dev/data-science/datasets/datasets-origin/dataset3_esp.csv\", sep=',') # Probando con ',' como separador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNDDOVQQ8MA-"
      },
      "source": [
        "<font color='lightgreen' size=12>Filtrar y explorar datasets</font>"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 1302,
=======
      "execution_count": 1283,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RESUMEN dataset2_es\n",
            "üìä Tama√±o del dataset: (2540, 2)\n",
            "üìä Registros √∫nicos: 2156\n",
            "üìä Sentimientos √∫nicos: 3\n",
            "üìä Registros duplicados: 298\n",
            "üìä Textos vac√≠os: 0\n",
            "üìä Sentimientos vac√≠os: 0\n",
            "üìä Registros duplicados: 298\n",
            "üìä Textos duplicados: 384\n",
            "                                                  texto sentimiento\n",
<<<<<<< HEAD
            "708                        Desconectado de todo, mejor.    negativo\n",
            "1637  Perd√≥n a aquellos por tenerlos colgados, realm...    positivo\n",
            "1253  Independientemente de las falencias estructura...    positivo\n",
=======
            "1377  [??BUS DETENIDO] Contin√∫a... En el km 35,1 sec...    positivo\n",
            "870   ¬øC√≥mo va su 2024? El m√≠o como enredado, pero a...    negativo\n",
            "257   primer treat del d√≠a (me merezco un mantecaito...    negativo\n",
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
            "--------------------------------------------------------------------------------\n",
            "\n",
            "RESUMEN dataset3_es\n",
            "üìä Tama√±o del dataset: (740, 2)\n",
            "üìä Registros √∫nicos: 697\n",
            "üìä Sentimientos √∫nicos: 3\n",
            "üìä Registros duplicados: 43\n",
            "üìä Textos vac√≠os: 0\n",
            "üìä Sentimientos vac√≠os: 0\n",
            "üìä Registros duplicados: 43\n",
            "üìä Textos duplicados: 43\n",
            "                                                 texto sentimiento\n",
<<<<<<< HEAD
            "406           Tengo los caballos en la parte de atr√°s.     Neutral\n",
            "187  Me alegra compartir el d√≠a del lanzamiento con...     Neutral\n",
            "507  El color de la camisa es un poco m√°s oscuro qu...     Neutral\n",
=======
            "269  Las pr√≥ximas GPU Tesla de Nvidia podr√≠an ser u...     Neutral\n",
            "404  Nueva oferta (Lanzamiento de juegos gratis en ...     Neutral\n",
            "386  Despu√©s de ver los nuevos videos de Leafy con ...     Neutral\n",
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"None of [Index(['texto', 'sentimiento'], dtype='object')] are in the [columns]\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
<<<<<<< HEAD
            "Cell \u001b[1;32mIn[1302], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m df2_filtrado_es \u001b[38;5;241m=\u001b[39m filtrar_dataset(df2_url_es,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset2_es\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m df3_filtrado_es \u001b[38;5;241m=\u001b[39m filtrar_dataset(df3_url_es,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset3_es\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m df1_filtrado_es \u001b[38;5;241m=\u001b[39m \u001b[43mfiltrar_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1_url_es\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset1_es\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[1302], line 3\u001b[0m, in \u001b[0;36mfiltrar_dataset\u001b[1;34m(data, nombre_dataset)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfiltrar_dataset\u001b[39m(data,nombre_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     data_filtro \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentimiento\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m     data_filtro \u001b[38;5;241m=\u001b[39m data_filtro[data_filtro[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexto\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRESUMEN\u001b[39m\u001b[38;5;124m'\u001b[39m,nombre_dataset)\n",
=======
            "Cell \u001b[1;32mIn[1283], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m df2_filtrado_es \u001b[38;5;241m=\u001b[39m filtrar_dataset(df2_url_es,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset2_es\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m df3_filtrado_es \u001b[38;5;241m=\u001b[39m filtrar_dataset(df3_url_es,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset3_es\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m df1_filtrado_es \u001b[38;5;241m=\u001b[39m \u001b[43mfiltrar_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1_url_es\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset1_es\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[1283], line 3\u001b[0m, in \u001b[0;36mfiltrar_dataset\u001b[1;34m(data, nombre_dataset)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfiltrar_dataset\u001b[39m(data,nombre_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     data_filtro \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentimiento\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m     data_filtro \u001b[38;5;241m=\u001b[39m data_filtro[data_filtro[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexto\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRESUMEN\u001b[39m\u001b[38;5;124m'\u001b[39m,nombre_dataset)\n",
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
            "File \u001b[1;32mc:\\Users\\marely\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\marely\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\marely\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['texto', 'sentimiento'], dtype='object')] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "# Funci√≥n filtrar dataset\n",
        "def filtrar_dataset(data,nombre_dataset='dataset'):\n",
        "    data_filtro = data[['texto', 'sentimiento']]\n",
        "    data_filtro = data_filtro[data_filtro['texto'].str.strip() != \"\"]\n",
        "\n",
        "    \n",
        "    print('\\nRESUMEN',nombre_dataset)\n",
        "    print(f\"üìä Tama√±o del dataset: {data_filtro.shape}\")\n",
        "    print(f\"üìä Registros √∫nicos: {data_filtro['texto'].nunique()}\")\n",
        "    print(f\"üìä Sentimientos √∫nicos: {data_filtro['sentimiento'].nunique()}\")\n",
        "    print(f\"üìä Registros duplicados: {data_filtro.duplicated().sum()}\")\n",
        "    print(f\"üìä Textos vac√≠os: {data_filtro['texto'].isnull().sum()}\")\n",
        "    print(f\"üìä Sentimientos vac√≠os: {data_filtro['sentimiento'].isnull().sum()}\")\n",
        "    print(f\"üìä Registros duplicados: {data_filtro.duplicated().sum()}\")\n",
        "    print(f\"üìä Textos duplicados: {data_filtro.duplicated(subset=['texto']).sum()}\")\n",
        "    \n",
        "    print(data_filtro.sample(3))\n",
        "    print('-' * 80)\n",
        "\n",
        "\n",
        "    return data_filtro\n",
        "\n",
        "# Reemplazar nombre columnas Text por texto, Sentiment por sentimiento\n",
        "df1_url_es.rename({'Text':'texto', 'Sentiment':'sentimiento'}, axis='columns', inplace=True)\n",
        "df2_filtrado_es = filtrar_dataset(df2_url_es,'dataset2_es')\n",
        "df3_filtrado_es = filtrar_dataset(df3_url_es,'dataset3_es')\n",
        "df1_filtrado_es = filtrar_dataset(df1_url_es,'dataset1_es')\n"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 1284,
>>>>>>> b91da02768ec751c81ecf7443b6aabce343aa149
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "execution_count": 1284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ver lista de columnas\n",
        "type(df1_url_es)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['abrumado', 'aburrimiento', 'aceptaci√≥n', 'admiraci√≥n', 'adoraci√≥n', 'agradecido', 'aislamiento', 'alegr√≠a', 'amabilidad', 'amargura', 'ambivalencia', 'amistad', 'amor', 'angustia', 'anhelo', 'ansiedad', 'anticipaci√≥n', 'apreciaci√≥n', 'aprensivo', 'armon√≠a', 'arrepentimiento', 'asco', 'asombro', 'cautivaci√≥n', 'celebraci√≥n', 'colorido', 'confiado', 'confianza', 'contentamiento', 'creatividad', 'cumplimiento', 'curiosidad', 'decepci√≥n', 'desamor', 'descubrimiento', 'desesperaci√≥n', 'deslumbrar', 'despectivo', 'determinaci√≥n', 'devastado', 'disfrute', 'diversi√≥n', 'dolor', 'elegancia', 'emoci√≥n', 'empoderamiento', 'emp√°tico', 'encantamiento', 'energ√≠a', 'enojo', 'entumecimiento', 'entusiasmo', 'envidia', 'envidioso', 'esperanza', 'euforia', 'excitaci√≥n', 'felicidad', 'frustraci√≥n', 'frustrado', 'grandeza', 'gratitud', 'inspiraci√≥n', 'inspirado', 'intimidaci√≥n', 'juguet√≥n', 'logro', 'l√°stima', 'malo', 'maravilla', 'melancol√≠a', 'mel√≥dico', 'miedo', 'motivaci√≥n', 'negativo', 'negativo', 'neutral', 'neutral', 'obst√°culo', 'odiar', 'optimismo', 'orgullo', 'pena', 'positividad', 'positivo', 'positivo', 'p√©rdida', 'reconfortante', 'reflexi√≥n', 'resentimiento', 'resiliencia', 'resplandor', 'reverencia', 'romance', 'satisfacci√≥n', 'serenidad', 'soledad', 'sorpresa', 'sufrimiento', 'temeroso', 'ternura', 'traici√≥n', 'tristeza', 'triunfo', 'verguenza', '√°nimo', '√©xito']\n",
            "Total de sentimientos: 107\n"
          ]
        }
      ],
      "source": [
        "#crear lista de sentimientos unicos y ordenados\n",
        "sentimientos_unicos_es = sorted(list([sentimiento.lower() for sentimiento in set(df1_filtrado_es['sentimiento'].unique()).union(set(df2_filtrado_es['sentimiento'].unique())).union(set(df3_filtrado_es['sentimiento'].unique()))]))\n",
        "\n",
        "print(sentimientos_unicos_es)\n",
        "print(f'Total de sentimientos: {len(sentimientos_unicos_es)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szn46SXAhzyW"
      },
      "source": [
        "### <font size=12 color=lightgreen>Limpiar textos</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppTw4PLfmrRx"
      },
      "source": [
        "#### **Funci√≥n para limpieza de textos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7pg4Upw97Ol"
      },
      "outputs": [],
      "source": [
        "def limpiar_texto_sentimientos(texto):\n",
        "    \"\"\"\n",
        "    Normaliza texto espa√±ol preservando √± y eliminando tildes.\n",
        "    NO convierte a min√∫sculas para preservar intensidad emocional.\n",
        "    \"\"\"\n",
        "    # Verifica si la entrada no es una cadena. Si no lo es, devuelve una cadena vac√≠a.\n",
        "    if not isinstance(texto, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Normaliza el texto para separar los caracteres base de sus diacr√≠ticos (ej., tildes).\n",
        "    texto = unicodedata.normalize('NFD', texto)\n",
        "\n",
        "    # 2. Reemplaza temporalmente las '√±' y '√ë' con marcadores especiales para preservarlas\n",
        "    # durante la eliminaci√≥n de diacr√≠ticos.\n",
        "    texto = texto.replace('n\\u0303', '@@@N_TILDE@@@')\n",
        "    texto = texto.replace('√±', '@@@N_TILDE@@@')\n",
        "    texto = texto.replace('N\\u0303', '@@@N_TILDE_MAYUS@@@')\n",
        "    texto = texto.replace('√ë', '@@@N_TILDE_MAYUS@@@')\n",
        "\n",
        "    # 3. Elimina los caracteres diacr√≠ticos (como las tildes) del texto.\n",
        "    texto = ''.join(\n",
        "        char for char in texto\n",
        "        if not unicodedata.combining(char)\n",
        "    )\n",
        "\n",
        "    # Restaura las '√±' y '√ë' utilizando los marcadores temporales.\n",
        "    texto = texto.replace('@@@N_TILDE@@@', '√±')\n",
        "    texto = texto.replace('@@@N_TILDE_MAYUS@@@', '√ë')\n",
        "\n",
        "    # Variable para almacenar el resultado de la limpieza.\n",
        "    resultado = texto\n",
        "    chars = []\n",
        "\n",
        "    # Itera sobre cada caracter en el resultado y a√±ade solo los caracteres imprimibles a una lista.\n",
        "    # Los caracteres no imprimibles (como los de control) son reemplazados por un espacio.\n",
        "    for char in resultado:\n",
        "        if char.isprintable():\n",
        "            chars.append(char)\n",
        "        else:\n",
        "            chars.append(' ')\n",
        "    resultado = ''.join(chars)\n",
        "\n",
        "    # Elimina URLs que terminan en \"...\" (posibles URLs rotas).\n",
        "    resultado = re.sub(r'https?://[^\\s]*\\.\\.\\.', '[URL_ROTA]', resultado)\n",
        "    resultado = re.sub(r'www\\.[^\\s]*\\\\.\\\\.\\\\.', '[URL_ROTA]', resultado)\n",
        "\n",
        "    # Normaliza los espacios m√∫ltiples a uno solo y elimina espacios al inicio y final.\n",
        "    resultado = ' '.join(resultado.split())\n",
        "    resultado = resultado.strip()\n",
        "\n",
        "\n",
        "    # Devuelve el texto preprocesado.\n",
        "    return resultado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "def obtener_lista_ordenada(*series_o_listas: Iterable,nombre='nombre'):\n",
        "    \"\"\"\n",
        "    Acepta cualquier cantidad de Series/Listas de sentimientos y devuelve\n",
        "    una lista ordenada de sentimientos √∫nicos ya limpios.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Unir todas las entradas en un solo iterable\n",
        "    todos = []\n",
        "    for s in series_o_listas:\n",
        "        todos.extend(list(s))\n",
        "\n",
        "\n",
        "    # 2) Limpiar y eliminar duplicados en un solo paso usando un set\n",
        "    sentimientos_limpios = {limpiar_texto_sentimientos(x) for x in todos}\n",
        "\n",
        "    print('\\n====> RESUMEN LIMPIEZA',nombre)\n",
        "    print(f'üìä Registros (original):',len(todos))\n",
        "    print(f'üìä Registros (despues de la limpieza):',len(sentimientos_limpios))\n",
        "    # listar \n",
        "    print(f'Lista {nombre} limpios: {', '.join(sentimientos_limpios)}')\n",
        "\n",
        "    print('-' * 80)\n",
        "\n",
        "    # 3) Devolver ordenado\n",
        "    return sorted(sentimientos_limpios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpieza dataframes\n",
        "\n",
        "def limpiar_dos_columnas(df, col1_name, col2_name, nombre_df=\"df\"):\n",
        "    \"\"\"\n",
        "    Aplica la limpieza a dos columnas de texto de un DataFrame\n",
        "    y devuelve un DataFrame nuevo con las columnas limpias.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Aplicar la funci√≥n limpiar_texto_sentimientos a ambas columnas\n",
        "    df_copy[col1_name + \"_limpio\"] = df_copy[col1_name].apply(limpiar_texto_sentimientos)\n",
        "    df_copy[col2_name + \"_limpio\"] = df_copy[col2_name].apply(limpiar_texto_sentimientos)\n",
        "\n",
        "    # Opcional: eliminar duplicados por las columnas limpias\n",
        "    df_unique = df_copy.drop_duplicates(subset=[col1_name + \"_limpio\", col2_name + \"_limpio\"])\n",
        "    print('\\n====> RESUMEN LIMPIEZA',nombre_df)\n",
        "    print(f\"üìä Filas en '{nombre_df}' (original): {len(df)}\")\n",
        "    print(f\"üìä Filas en '{nombre_df}' (√∫nicas por columnas limpias): {len(df_unique)}\")\n",
        "    print(df_unique[[col1_name, col2_name, col1_name + \"_limpio\", col2_name + \"_limpio\"]].head(3))\n",
        "    print('-' * 80)\n",
        "\n",
        "    return df_unique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYkBr6o-8MBA",
        "outputId": "a80b78a8-4d49-4164-d9cd-9918feb82b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====> RESUMEN LIMPIEZA positivos\n",
            "üìä Registros (original): 1\n",
            "üìä Registros (despues de la limpieza): 1\n",
            "Lista positivos limpios: positivo\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====> RESUMEN LIMPIEZA negativos\n",
            "üìä Registros (original): 1\n",
            "üìä Registros (despues de la limpieza): 1\n",
            "Lista negativos limpios: negativo\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====> RESUMEN LIMPIEZA neutros\n",
            "üìä Registros (original): 1\n",
            "üìä Registros (despues de la limpieza): 1\n",
            "Lista neutros limpios: neutral\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====> RESUMEN LIMPIEZA lista sentimientos_unicos\n",
            "üìä Registros (original): 107\n",
            "üìä Registros (despues de la limpieza): 104\n",
            "Lista lista sentimientos_unicos limpios: romance, empatico, elegancia, satisfaccion, lastima, enojo, entusiasmo, envidioso, creatividad, abrumado, ambivalencia, arrepentimiento, positivo, temeroso, decepcion, asombro, inspiracion, celebracion, euforia, exito, soledad, anticipacion, gratitud, amabilidad, grandeza, melodico, envidia, inspirado, empoderamiento, animo, alegria, devastado, desesperacion, negativo, confianza, frustracion, dolor, pena, neutral, adoracion, colorido, disfrute, ternura, descubrimiento, reverencia, reflexion, tristeza, emocion, apreciacion, cautivacion, energia, orgullo, resiliencia, amargura, entumecimiento, maravilla, anhelo, jugueton, motivacion, melancolia, amistad, frustrado, agradecido, despectivo, admiracion, intimidacion, deslumbrar, traicion, curiosidad, asco, excitacion, diversion, amor, armonia, angustia, contentamiento, desamor, malo, resentimiento, odiar, encantamiento, confiado, determinacion, aislamiento, logro, serenidad, esperanza, aprensivo, sorpresa, positividad, ansiedad, aburrimiento, perdida, resplandor, felicidad, verguenza, miedo, cumplimiento, obstaculo, triunfo, sufrimiento, optimismo, aceptacion, reconfortante\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====> RESUMEN LIMPIEZA dataset1_es\n",
            "üìä Filas en 'dataset1_es' (original): 731\n",
            "üìä Filas en 'dataset1_es' (√∫nicas por columnas limpias): 708\n",
            "                                              texto sentimiento  \\\n",
            "0      ¬°Disfrutando de un hermoso d√≠a en el parque!    Positivo   \n",
            "1              Esta ma√±ana el tr√°fico era terrible.    Negativo   \n",
            "2  ¬°Acabo de terminar un entrenamiento incre√≠ble!??    Positivo   \n",
            "\n",
            "                                       texto_limpio sentimiento_limpio  \n",
            "0      ¬°Disfrutando de un hermoso dia en el parque!           Positivo  \n",
            "1              Esta ma√±ana el trafico era terrible.           Negativo  \n",
            "2  ¬°Acabo de terminar un entrenamiento increible!??           Positivo  \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====> RESUMEN LIMPIEZA dataset2_es\n",
            "üìä Filas en 'dataset2_es' (original): 2540\n",
            "üìä Filas en 'dataset2_es' (√∫nicas por columnas limpias): 2230\n",
            "                                               texto sentimiento  \\\n",
            "0               termine bien abrumado despu√©s de hoy    negativo   \n",
            "1                                 me siento abrumado    negativo   \n",
            "2  Me siento un poco abrumado por la cantidad de ...    negativo   \n",
            "\n",
            "                                        texto_limpio sentimiento_limpio  \n",
            "0               termine bien abrumado despues de hoy           negativo  \n",
            "1                                 me siento abrumado           negativo  \n",
            "2  Me siento un poco abrumado por la cantidad de ...           negativo  \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====> RESUMEN LIMPIEZA dataset3_es\n",
            "üìä Filas en 'dataset3_es' (original): 740\n",
            "üìä Filas en 'dataset3_es' (√∫nicas por columnas limpias): 695\n",
            "                                               texto sentimiento  \\\n",
            "0  La Varlope, dura como una roca, rara y poderos...     Neutral   \n",
            "1  Rock en vivo - M√∫sica dura La la Varlope, RARE...     Neutral   \n",
            "2  Soy duro como yo, RARE LONDON DE, HANDSOME 201...     Neutral   \n",
            "\n",
            "                                        texto_limpio sentimiento_limpio  \n",
            "0  La Varlope, dura como una roca, rara y poderos...            Neutral  \n",
            "1  Rock en vivo - Musica dura La la Varlope, RARE...            Neutral  \n",
            "2  Soy duro como yo, RARE LONDON DE, HANDSOME 201...            Neutral  \n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#Listas\n",
        "positivos_limpios_es = obtener_lista_ordenada(positivos_es, nombre='positivos')\n",
        "negativos_limpios_es = obtener_lista_ordenada(negativos_es, nombre='negativos')\n",
        "neutros_limpios_es = obtener_lista_ordenada(neutros_es, nombre='neutros')\n",
        "sentimientos_unicos_limpios_es = obtener_lista_ordenada(sentimientos_unicos_es, nombre='lista sentimientos_unicos')\n",
        "\n",
        "# Uso dataframe\n",
        "df1_limpio_es = limpiar_dos_columnas(df1_filtrado_es, \"texto\", \"sentimiento\", nombre_df='dataset1_es')\n",
        "df2_limpio_es = limpiar_dos_columnas(df2_filtrado_es, \"texto\", \"sentimiento\", nombre_df='dataset2_es')\n",
        "df3_limpio_es = limpiar_dos_columnas(df3_filtrado_es, \"texto\", \"sentimiento\", nombre_df='dataset3_es')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=lightgreen size=12>Unificar datasets</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====> RESUMEN DATASET UNIFICADO\n",
            "Registros dataset1_es: 708      Porcentaje: 19.49%\n",
            "Registros dataset2_es: 2230     Porcentaje: 61.38%\n",
            "Registros dataset3_es: 695      Porcentaje: 19.13%\n",
            "Registros dataset_unificado: 3633\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto_limpio</th>\n",
              "      <th>sentimiento_limpio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2526</th>\n",
              "      <td>. ¬øDe que murio? - Se asfixio con el celo envo...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2120</th>\n",
              "      <td>seguro esta en el campo por eso no sube pavada</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>La soledad aparece a medida que la noche se vu...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>Compasion en accion: apoyar un evento benefico...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2949</th>\n",
              "      <td>Nvidia confirmada en fundiciones globales: TSM...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           texto_limpio sentimiento_limpio\n",
              "2526  . ¬øDe que murio? - Se asfixio con el celo envo...            neutral\n",
              "2120     seguro esta en el campo por eso no sube pavada           positivo\n",
              "165   La soledad aparece a medida que la noche se vu...            Neutral\n",
              "116   Compasion en accion: apoyar un evento benefico...            Neutral\n",
              "2949  Nvidia confirmada en fundiciones globales: TSM...            Neutral"
            ]
          },
          "execution_count": 1217,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Unificar dataset\n",
        "df_unificado_es = pd.concat([df1_limpio_es[['texto_limpio','sentimiento_limpio']], df2_limpio_es[['texto_limpio','sentimiento_limpio']], df3_limpio_es[['texto_limpio','sentimiento_limpio']]], ignore_index=True)\n",
        "\n",
        "# Estad√≠sticas b√°sicas del dataset unificado\n",
        "# Porcentaje con dos decimales\n",
        "\n",
        "print(\"====> RESUMEN DATASET UNIFICADO\")\n",
        "\n",
        "print(f\"Registros dataset1_es: {len(df1_limpio_es)}      Porcentaje: {(len(df1_limpio_es)/len(df_unificado_es))*100:.2f}%\")\n",
        "print(f\"Registros dataset2_es: {len(df2_limpio_es)}     Porcentaje: {(len(df2_limpio_es)/len(df_unificado_es))*100:.2f}%\")\n",
        "print(f\"Registros dataset3_es: {len(df3_limpio_es)}      Porcentaje: {(len(df3_limpio_es)/len(df_unificado_es))*100:.2f}%\")\n",
        "print(f\"Registros dataset_unificado: {len(df_unificado_es)}\")\n",
        "df_unificado_es.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYvX17ceGa1i"
      },
      "source": [
        "### <font size=12 color=lightgreen>Categorizar de sentimientos </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzQa-9sE8MBB"
      },
      "source": [
        "#### **Funci√≥n para categorizar sentimientos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALbr-iTw8MBB"
      },
      "outputs": [],
      "source": [
        "def categorizar_sentimiento(sentimiento, categorias):\n",
        "    \"\"\"\n",
        "    Categoriza sentimientos solo si est√°n en las listas definidas.\n",
        "    Devuelve None para sentimientos no clasificados.\n",
        "    \"\"\"\n",
        "    sent = str(sentimiento).strip().lower()\n",
        "    if sent in positivos_limpios_es:\n",
        "        return 'positivo'\n",
        "    elif sent in negativos_limpios_es:\n",
        "        return 'negativo'\n",
        "    elif sent in neutros_limpios_es:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        # Devolvemos None para posterior filtrado\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalizar datasets\n",
        "def normalizar_dataset(dataset):\n",
        "\t# Crea una copia con un dataset excluyendo valores nulos\n",
        "\tnormalizado = dataset[dataset['sentimiento_final'].notna()].copy()\n",
        "\n",
        "\t# Renombrar columnas\n",
        "\tnormalizado = normalizado.rename(columns={'texto_limpio': 'texto', 'sentimiento_final': 'sentimiento'})\n",
        "\n",
        "\t# Quitar la columna sentimiento_limpio\n",
        "\tnormalizado = normalizado.drop(columns=['sentimiento_limpio'])\n",
        "\n",
        "\treturn normalizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMKuIMHg8MBC"
      },
      "source": [
        "#### **Categorizar sentimientos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ df_normalizado_es: 3169 registros categorizados\n"
          ]
        }
      ],
      "source": [
        "categorias = [positivos_limpios_es, negativos_limpios_es, neutros_limpios_es]\n",
        "\n",
        "\n",
        "# catogorizar sentimientos\n",
        "df_unificado_es['sentimiento_final'] = df_unificado_es['sentimiento_limpio'].apply(\n",
        "    lambda x: categorizar_sentimiento(x,categorias))\n",
        "\n",
        "# Normalizar dataset\n",
        "df_normalizado_es = normalizar_dataset(df_unificado_es)\n",
        "\n",
        "print(f\"‚úÖ df_normalizado_es: {len(df_normalizado_es)} registros categorizados\")\n",
        "\n",
        "#df_normalizado_es.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "gq2bsqV_8MBC",
        "outputId": "6110aa71-c8d3-4602-e0da-be730317b416"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimiento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>¬°Disfrutando de un hermoso dia en el parque!</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Esta ma√±ana el trafico era terrible.</td>\n",
              "      <td>negativo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>¬°Acabo de terminar un entrenamiento increible!??</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>¬°Emocionado por la escapada de fin de semana q...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Probando una nueva receta para cenar esta noche.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3628</th>\n",
              "      <td>Vida.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3629</th>\n",
              "      <td>SolA a a‚Ç¨‚Äπa‚Ç¨‚Äπpensar que Ellison era un buen ti...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3630</th>\n",
              "      <td>SolA a a‚Ç¨‚Äπa‚Ç¨‚Äπpensar que Ellison era un buen ti...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3631</th>\n",
              "      <td>SolA a a‚Ç¨‚Äπa‚Ç¨‚Äπpensar que Ellison era un tipo no...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3632</th>\n",
              "      <td>Yo tendia a pensar que Ellison era un buen tip...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3169 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  texto sentimiento\n",
              "0          ¬°Disfrutando de un hermoso dia en el parque!    positivo\n",
              "1                  Esta ma√±ana el trafico era terrible.    negativo\n",
              "2      ¬°Acabo de terminar un entrenamiento increible!??    positivo\n",
              "3     ¬°Emocionado por la escapada de fin de semana q...    positivo\n",
              "4      Probando una nueva receta para cenar esta noche.     neutral\n",
              "...                                                 ...         ...\n",
              "3628                                              Vida.     neutral\n",
              "3629  SolA a a‚Ç¨‚Äπa‚Ç¨‚Äπpensar que Ellison era un buen ti...     neutral\n",
              "3630  SolA a a‚Ç¨‚Äπa‚Ç¨‚Äπpensar que Ellison era un buen ti...     neutral\n",
              "3631  SolA a a‚Ç¨‚Äπa‚Ç¨‚Äπpensar que Ellison era un tipo no...     neutral\n",
              "3632  Yo tendia a pensar que Ellison era un buen tip...     neutral\n",
              "\n",
              "[3169 rows x 2 columns]"
            ]
          },
          "execution_count": 1221,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_normalizado_es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwBnt_Bx8MBD"
      },
      "source": [
        "### <font color=lightgreen size=12>Limpiar dataset unificado</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPIqj6G-8MBD"
      },
      "source": [
        "#### **Funci√≥n limpieza dataset unificado**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJD_1mN88MBD"
      },
      "outputs": [],
      "source": [
        "def limpiar_dataset(data, verbose=True):\n",
        "    \"\"\"\n",
        "    Limpia dataset unificado para an√°lisis de sentimientos.\n",
        "\n",
        "    Proceso:\n",
        "    1. Identifica y elimina CONTRADICCIONES (textos con diferentes sentimientos)\n",
        "    2. Elimina DUPLICADOS exactos (mismo texto, mismo sentimiento)\n",
        "    3. Limpieza final (espacios vac√≠os, NaN)\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame con 'Texto_Limpio' y 'Sentimiento_Final'\n",
        "        verbose: Si True, muestra an√°lisis detallado\n",
        "\n",
        "    Returns:\n",
        "        DataFrame limpio, sin duplicados ni contradicciones\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(\"üßπ LIMPIANDO DATASET UNIFICADO\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Registros iniciales: {len(data):,}\")\n",
        "        print(f\"Textos √∫nicos iniciales: {data['texto'].nunique():,}\")\n",
        "\n",
        "    # Hacer copia para no modificar original\n",
        "    df = data.copy()\n",
        "\n",
        "    # ===== 1. ELIMINAR CONTRADICCIONES (PRIMERO) =====\n",
        "    if verbose:\n",
        "        print(f\"\\n1. üîç BUSCANDO CONTRADICCIONES...\")\n",
        "\n",
        "    # Textos con m√°s de un sentimiento diferente\n",
        "    conteo_sentimientos = df.groupby('texto')['sentimiento'].nunique()\n",
        "    textos_con_contradiccion = conteo_sentimientos[conteo_sentimientos > 1].index.tolist()\n",
        "\n",
        "    if textos_con_contradiccion:\n",
        "        if verbose:\n",
        "            print(f\"   ‚ö†Ô∏è  Encontradas: {len(textos_con_contradiccion):,} contradicciones\")\n",
        "\n",
        "            # Mostrar algunos ejemplos\n",
        "            print(f\"   ‚Ä¢ Ejemplos (primeros 2):\")\n",
        "            for texto in textos_con_contradiccion[:2]:\n",
        "                sentimientos = df[df['texto'] == texto]['sentimiento'].unique()\n",
        "                texto_corto = texto[:60] + \"...\" if len(texto) > 60 else texto\n",
        "                print(f\"     - '{texto_corto}'\")\n",
        "                print(f\"       ‚Üí Sentimientos: {', '.join(sentimientos)}\")\n",
        "\n",
        "        # Eliminar TODOS los registros de textos contradictorios\n",
        "        df_sin_contradicciones = df[~df['texto'].isin(textos_con_contradiccion)].copy()\n",
        "\n",
        "        if verbose:\n",
        "            eliminados = len(df) - len(df_sin_contradicciones)\n",
        "            print(f\"   üóëÔ∏è  Eliminados: {eliminados:,} registros por contradicciones\")\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(f\"   ‚úÖ No hay contradicciones\")\n",
        "        df_sin_contradicciones = df.copy()\n",
        "\n",
        "    # ===== 2. ELIMINAR DUPLICADOS EXACTOS =====\n",
        "    if verbose:\n",
        "        print(f\"\\n2. üîç BUSCANDO DUPLICADOS EXACTOS...\")\n",
        "\n",
        "    # Contar duplicados exactos (mismo texto, mismo sentimiento)\n",
        "    conteo_duplicados = df_sin_contradicciones['texto'].value_counts()\n",
        "    textos_duplicados = conteo_duplicados[conteo_duplicados > 1].index.tolist()\n",
        "\n",
        "    if textos_duplicados:\n",
        "        if verbose:\n",
        "            print(f\"   ‚ö†Ô∏è  Encontrados: {len(textos_duplicados):,} textos duplicados\")\n",
        "\n",
        "            # Calcular cu√°ntos registros se eliminar√°n\n",
        "            total_a_eliminar = sum([conteo_duplicados[t] - 1 for t in textos_duplicados])\n",
        "            print(f\"   ‚Ä¢ Registros a eliminar: {total_a_eliminar:,}\")\n",
        "\n",
        "        # Eliminar duplicados (mantener primera aparici√≥n)\n",
        "        df_sin_duplicados = df_sin_contradicciones.drop_duplicates(\n",
        "            subset=['texto'],\n",
        "            keep='first'\n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            eliminados = len(df_sin_contradicciones) - len(df_sin_duplicados)\n",
        "            print(f\"   üóëÔ∏è  Eliminados: {eliminados:,} registros duplicados\")\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(f\"   ‚úÖ No hay duplicados exactos\")\n",
        "        df_sin_duplicados = df_sin_contradicciones.copy()\n",
        "\n",
        "    # ===== 3. LIMPIEZA FINAL =====\n",
        "    if verbose:\n",
        "        print(f\"\\n3. üßπ LIMPIEZA FINAL...\")\n",
        "\n",
        "    df_final = df_sin_duplicados.copy()\n",
        "\n",
        "    # Filtrar solo columnas necesarias\n",
        "    df_final = df_final[['texto', 'sentimiento']]\n",
        "\n",
        "    # Eliminar textos vac√≠os o solo espacios\n",
        "    textos_vacios_antes = len(df_final)\n",
        "    df_final = df_final[df_final['texto'].str.strip() != \"\"]\n",
        "    textos_vacios_eliminados = textos_vacios_antes - len(df_final)\n",
        "\n",
        "    if verbose and textos_vacios_eliminados > 0:\n",
        "        print(f\"   ‚Ä¢ Textos vac√≠os eliminados: {textos_vacios_eliminados}\")\n",
        "\n",
        "    # Eliminar sentimientos NaN\n",
        "    sentimientos_nan_antes = len(df_final)\n",
        "    df_final = df_final[df_final['sentimiento'].notna()]\n",
        "    sentimientos_nan_eliminados = sentimientos_nan_antes - len(df_final)\n",
        "\n",
        "    if verbose and sentimientos_nan_eliminados > 0:\n",
        "        print(f\"   ‚Ä¢ Sentimientos NaN eliminados: {sentimientos_nan_eliminados}\")\n",
        "\n",
        "    # ===== 4. VERIFICACI√ìN Y RESUMEN =====\n",
        "    if verbose:\n",
        "        print(f\"\\n4. ‚úÖ VERIFICACI√ìN FINAL\")\n",
        "        print(f\"   ‚Ä¢ Registros finales: {len(df_final):,}\")\n",
        "        print(f\"   ‚Ä¢ Textos √∫nicos finales: {df_final['texto'].nunique():,}\")\n",
        "\n",
        "        # Verificar que cada texto aparece solo una vez\n",
        "        if len(df_final) == df_final['texto'].nunique():\n",
        "            print(f\"   üéØ ¬°Dataset 100% limpio! Cada texto aparece solo una vez\")\n",
        "        else:\n",
        "            diferencia = len(df_final) - df_final['texto'].nunique()\n",
        "            print(f\"   ‚ö†Ô∏è  ¬°Problema! Hay {diferencia} duplicados\")\n",
        "\n",
        "        # Resumen\n",
        "        print(f\"\\n\" + \"=\" * 50)\n",
        "        print(\"üìä RESUMEN DE LIMPIEZA\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        total_eliminados = (len(data) - len(df_final))\n",
        "        porcentaje_eliminado = (total_eliminados / len(data)) * 100\n",
        "\n",
        "        print(f\"Registros iniciales: {len(data):,}\")\n",
        "        print(f\"Registros finales: {len(df_final):,}\")\n",
        "        print(f\"Total eliminados: {total_eliminados:,} ({porcentaje_eliminado:.1f}%)\")\n",
        "\n",
        "        # Distribuci√≥n de sentimientos\n",
        "        print(f\"\\nüìà DISTRIBUCI√ìN FINAL DE SENTIMIENTOS:\")\n",
        "        distribucion = df_final['sentimiento'].value_counts()\n",
        "        for sentimiento, count in distribucion.items():\n",
        "            porcentaje = (count / len(df_final)) * 100\n",
        "            print(f\"   ‚Ä¢ {sentimiento}: {count:,} ({porcentaje:.1f}%)\")\n",
        "\n",
        "    return df_final\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geihh0678MBE",
        "outputId": "93fca709-a0d1-4cfe-d6ce-e30d9a469b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üîó DATASET FINAL CATEGORIZADO\n",
            "======================================================================\n",
            "üì¶ Dataset unificado: (3169, 2)\n",
            "   ‚Ä¢ Registros: 3,169\n",
            "   ‚Ä¢ Textos √∫nicos: 3,080\n",
            "\n",
            "======================================================================\n",
            "üßπ APLICANDO LIMPIEZA AL DATASET UNIFICADO\n",
            "======================================================================\n",
            "üßπ LIMPIANDO DATASET UNIFICADO\n",
            "--------------------------------------------------\n",
            "Registros iniciales: 3,169\n",
            "Textos √∫nicos iniciales: 3,080\n",
            "\n",
            "1. üîç BUSCANDO CONTRADICCIONES...\n",
            "   ‚ö†Ô∏è  Encontradas: 89 contradicciones\n",
            "   ‚Ä¢ Ejemplos (primeros 2):\n",
            "     - '\"De manera apacible, se puede sacudir el mundo\" MG'\n",
            "       ‚Üí Sentimientos: negativo, positivo\n",
            "     - '\"He aprendido que el valor no es la ausencia de miedo, sino ...'\n",
            "       ‚Üí Sentimientos: neutral, positivo\n",
            "   üóëÔ∏è  Eliminados: 178 registros por contradicciones\n",
            "\n",
            "2. üîç BUSCANDO DUPLICADOS EXACTOS...\n",
            "   ‚úÖ No hay duplicados exactos\n",
            "\n",
            "3. üßπ LIMPIEZA FINAL...\n",
            "\n",
            "4. ‚úÖ VERIFICACI√ìN FINAL\n",
            "   ‚Ä¢ Registros finales: 2,991\n",
            "   ‚Ä¢ Textos √∫nicos finales: 2,991\n",
            "   üéØ ¬°Dataset 100% limpio! Cada texto aparece solo una vez\n",
            "\n",
            "==================================================\n",
            "üìä RESUMEN DE LIMPIEZA\n",
            "==================================================\n",
            "Registros iniciales: 3,169\n",
            "Registros finales: 2,991\n",
            "Total eliminados: 178 (5.6%)\n",
            "\n",
            "üìà DISTRIBUCI√ìN FINAL DE SENTIMIENTOS:\n",
            "   ‚Ä¢ neutral: 1,115 (37.3%)\n",
            "   ‚Ä¢ negativo: 967 (32.3%)\n",
            "   ‚Ä¢ positivo: 909 (30.4%)\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"üîó DATASET FINAL CATEGORIZADO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"üì¶ Dataset unificado: {df_normalizado_es.shape}\")\n",
        "print(f\"   ‚Ä¢ Registros: {len(df_normalizado_es):,}\")\n",
        "print(f\"   ‚Ä¢ Textos √∫nicos: {df_normalizado_es['texto'].nunique():,}\")\n",
        "\n",
        "\n",
        "# %%\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üßπ APLICANDO LIMPIEZA AL DATASET UNIFICADO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Aplicar limpieza\n",
        "\n",
        "df_final_es = limpiar_dataset(df_normalizado_es, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "_bMK2Nt78MBE",
        "outputId": "a4c5bc6a-904b-4e5b-f121-aaf2974d4b28"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimiento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1911</th>\n",
              "      <td>LUCIA CON VOZ DE GLOBO DE HELIO JAJAJAJ TE AMO...</td>\n",
              "      <td>positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3255</th>\n",
              "      <td>NVIDIA lanzo una actualizacion de seguridad pa...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3026</th>\n",
              "      <td>Siguen llegando noticias: @Tesla no vale mA¬°s ...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  texto sentimiento\n",
              "1911  LUCIA CON VOZ DE GLOBO DE HELIO JAJAJAJ TE AMO...    positivo\n",
              "3255  NVIDIA lanzo una actualizacion de seguridad pa...     neutral\n",
              "3026  Siguen llegando noticias: @Tesla no vale mA¬°s ...     neutral"
            ]
          },
          "execution_count": 1224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_final_es.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gya1CknQ8MBE"
      },
      "source": [
        " ### <font size=12 color=lightgreen>An√°lisis de Distribuci√≥n y Visualizaci√≥n</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcLXDcSo8MBE"
      },
      "source": [
        "#### **An√°lisis de distribuci√≥n de sentimientos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NvujxiH8MBE",
        "outputId": "18e075f1-421f-4ddf-ace8-6bcb7776d3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üìà AN√ÅLISIS DE DISTRIBUCI√ìN - DATASET FINAL\n",
            "============================================================\n",
            "SENTIMIENTO  | CANTIDAD | PORCENTAJE | PROPORCI√ìN\n",
            "--------------------------------------------------\n",
            "Positivo     |      909 |     30.39% | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "Negativo     |      967 |     32.33% | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "Neutral      |     1115 |     37.28% | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "--------------------------------------------------\n",
            "TOTAL        |     2991 |    100.00% | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#üìä AN√ÅLISIS DE DISTRIBUCI√ìN DEL DATASET FINAL\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìà AN√ÅLISIS DE DISTRIBUCI√ìN - DATASET FINAL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Calcular conteos y porcentajes\n",
        "conteos = df_final_es['sentimiento'].value_counts()\n",
        "total_registros = len(df_final_es)\n",
        "porcentajes = (conteos / total_registros * 100).round(2)\n",
        "\n",
        "# 2. Mostrar tabla detallada\n",
        "print(f\"{'SENTIMIENTO':<12} | {'CANTIDAD':>8} | {'PORCENTAJE':>10} | {'PROPORCI√ìN'}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for sentimiento in ['positivo', 'negativo', 'neutral']:\n",
        "    if sentimiento in conteos:\n",
        "        count = conteos[sentimiento]\n",
        "        porcentaje = porcentajes[sentimiento]\n",
        "        # Crear barra visual\n",
        "        barra = '‚ñà' * int(count / total_registros * 40)  # Escala a 40 caracteres\n",
        "        print(f\"{sentimiento.capitalize():<12} | {count:>8} | {porcentaje:>9}% | {barra}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'TOTAL':<12} | {total_registros:>8} | {'100.00':>9}% | {'‚ñà' * 40}\")\n",
        "print(\"-\" * 58)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlCa1cj_8MBF"
      },
      "source": [
        "#### **Visualizaci√≥n de la distribuci√≥n de Sentimientos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "HWMqTdvf8MBF",
        "outputId": "921237fc-ba25-482d-88a8-9500bd1875a9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.plotly.v1+json": {
              "config": {
                "plotlyServerURL": "https://plot.ly"
              },
              "data": [
                {
                  "domain": {
                    "x": [
                      0,
                      1
                    ],
                    "y": [
                      0,
                      1
                    ]
                  },
                  "hovertemplate": "label=%{label}<br>value=%{value}<extra></extra>",
                  "insidetextfont": {
                    "color": "white",
                    "size": 14
                  },
                  "labels": [
                    "neutral",
                    "negativo",
                    "positivo"
                  ],
                  "legendgroup": "",
                  "name": "",
                  "showlegend": true,
                  "textinfo": "label+percent",
                  "textposition": "inside",
                  "type": "pie",
                  "values": {
                    "bdata": "WwTHA40D",
                    "dtype": "i2"
                  }
                }
              ],
              "layout": {
                "height": 500,
                "legend": {
                  "tracegroupgap": 0
                },
                "margin": {
                  "t": 60
                },
                "showlegend": false,
                "template": {
                  "data": {
                    "bar": [
                      {
                        "error_x": {
                          "color": "#2a3f5f"
                        },
                        "error_y": {
                          "color": "#2a3f5f"
                        },
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "bar"
                      }
                    ],
                    "barpolar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "barpolar"
                      }
                    ],
                    "carpet": [
                      {
                        "aaxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "baxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "type": "carpet"
                      }
                    ],
                    "choropleth": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "choropleth"
                      }
                    ],
                    "contour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "contour"
                      }
                    ],
                    "contourcarpet": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contourcarpet"
                      }
                    ],
                    "heatmap": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "heatmap"
                      }
                    ],
                    "histogram": [
                      {
                        "marker": {
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "histogram"
                      }
                    ],
                    "histogram2d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2d"
                      }
                    ],
                    "histogram2dcontour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2dcontour"
                      }
                    ],
                    "mesh3d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "mesh3d"
                      }
                    ],
                    "parcoords": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "parcoords"
                      }
                    ],
                    "pie": [
                      {
                        "automargin": true,
                        "type": "pie"
                      }
                    ],
                    "scatter": [
                      {
                        "fillpattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        },
                        "type": "scatter"
                      }
                    ],
                    "scatter3d": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter3d"
                      }
                    ],
                    "scattercarpet": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattercarpet"
                      }
                    ],
                    "scattergeo": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergeo"
                      }
                    ],
                    "scattergl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergl"
                      }
                    ],
                    "scattermap": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermap"
                      }
                    ],
                    "scattermapbox": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermapbox"
                      }
                    ],
                    "scatterpolar": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolar"
                      }
                    ],
                    "scatterpolargl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolargl"
                      }
                    ],
                    "scatterternary": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterternary"
                      }
                    ],
                    "surface": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "surface"
                      }
                    ],
                    "table": [
                      {
                        "cells": {
                          "fill": {
                            "color": "#EBF0F8"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "header": {
                          "fill": {
                            "color": "#C8D4E3"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "type": "table"
                      }
                    ]
                  },
                  "layout": {
                    "annotationdefaults": {
                      "arrowcolor": "#2a3f5f",
                      "arrowhead": 0,
                      "arrowwidth": 1
                    },
                    "autotypenumbers": "strict",
                    "coloraxis": {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      }
                    },
                    "colorscale": {
                      "diverging": [
                        [
                          0,
                          "#8e0152"
                        ],
                        [
                          0.1,
                          "#c51b7d"
                        ],
                        [
                          0.2,
                          "#de77ae"
                        ],
                        [
                          0.3,
                          "#f1b6da"
                        ],
                        [
                          0.4,
                          "#fde0ef"
                        ],
                        [
                          0.5,
                          "#f7f7f7"
                        ],
                        [
                          0.6,
                          "#e6f5d0"
                        ],
                        [
                          0.7,
                          "#b8e186"
                        ],
                        [
                          0.8,
                          "#7fbc41"
                        ],
                        [
                          0.9,
                          "#4d9221"
                        ],
                        [
                          1,
                          "#276419"
                        ]
                      ],
                      "sequential": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "sequentialminus": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ]
                    },
                    "colorway": [
                      "#636efa",
                      "#EF553B",
                      "#00cc96",
                      "#ab63fa",
                      "#FFA15A",
                      "#19d3f3",
                      "#FF6692",
                      "#B6E880",
                      "#FF97FF",
                      "#FECB52"
                    ],
                    "font": {
                      "color": "#2a3f5f"
                    },
                    "geo": {
                      "bgcolor": "white",
                      "lakecolor": "white",
                      "landcolor": "#E5ECF6",
                      "showlakes": true,
                      "showland": true,
                      "subunitcolor": "white"
                    },
                    "hoverlabel": {
                      "align": "left"
                    },
                    "hovermode": "closest",
                    "mapbox": {
                      "style": "light"
                    },
                    "paper_bgcolor": "white",
                    "plot_bgcolor": "#E5ECF6",
                    "polar": {
                      "angularaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "radialaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "scene": {
                      "xaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "yaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "zaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      }
                    },
                    "shapedefaults": {
                      "line": {
                        "color": "#2a3f5f"
                      }
                    },
                    "ternary": {
                      "aaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "baxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "caxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "title": {
                      "x": 0.05
                    },
                    "xaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    },
                    "yaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    }
                  }
                },
                "title": {
                  "text": "<b>Distribuci√≥n de Sentimientos</b><br><span style=\"font-size:14px\">Dataset Final: 2991 registros</span>",
                  "x": 0.5
                },
                "width": 500
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Grafica de pastel con Plotly\n",
        "\n",
        "valores = df_final_es['sentimiento'].value_counts().reset_index()\n",
        "valores.columns = ['sentimientos', 'Cantidad']\n",
        "fig1 = px.pie(\n",
        "    names = valores.sentimientos,\n",
        "    values = valores.Cantidad,\n",
        ")\n",
        "\n",
        "fig1.update_traces(textposition='inside', textinfo='label+percent',  insidetextfont=dict(color = 'white', size=14)\n",
        ")\n",
        "\n",
        "fig1.update_layout(\n",
        "    title_text=f'<b>Distribuci√≥n de Sentimientos</b><br><span style=\"font-size:14px\">Dataset Final: {total_registros} registros</span>',\n",
        "    title_x=0.5,\n",
        "    width=500,\n",
        "    height=500,\n",
        "    showlegend=False,\n",
        ")\n",
        "\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruYdbk2Q8MBF"
      },
      "source": [
        "### <font size=12 color=lightgreen> Exportar dataset </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIGvLppq8MBF"
      },
      "source": [
        "#### **Definir ruta de exportaci√≥n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmfUY-ca8MBF"
      },
      "outputs": [],
      "source": [
        "# Ruta actual\n",
        "ruta_actual = Path.cwd()\n",
        "\n",
        "# Buscar data-science\n",
        "if ruta_actual.name == 'notebooks':\n",
        "    # Si estamos en notebooks/, ir a ../datasets\n",
        "    carpeta_datasets = ruta_actual.parent / 'datasets'\n",
        "else:\n",
        "    # Buscar data-science en directorios padres\n",
        "    for directorio_padre in ruta_actual.parents:\n",
        "        if (directorio_padre / 'data-science').exists():\n",
        "            carpeta_datasets = directorio_padre / 'data-science' / 'datasets'\n",
        "            break\n",
        "    else:\n",
        "        # Si no encuentra, usar directorio actual/datasets\n",
        "        carpeta_datasets = ruta_actual / 'datasets'\n",
        "\n",
        "# Crear carpeta si no existe\n",
        "carpeta_datasets.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Ruta completa del archivo\n",
        "archivo_final = carpeta_datasets / 'dataset_listo_para_ML.csv'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiHQF1Bk8MBF"
      },
      "source": [
        "#### **Exportar dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svaz0jBZ8MBF",
        "outputId": "09af4c32-24f7-4dbf-ca63-52f9ce06d62a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset exportado: c:\\Users\\marely\\OneDrive\\Documentos\\Oracle_ONE\\Hackaton\\SentimentAPI-Project\\sentiment-api\\data-science\\datasets\\dataset_listo_para_ML.csv\n",
            "üìä Registros: 2,991\n"
          ]
        }
      ],
      "source": [
        "# Renombrar columnas para formato final\n",
        "df_exportar = df_final_es.rename({\n",
        "    'Texto_Limpio': 'texto',\n",
        "    'Sentimiento_Final': 'sentimiento'\n",
        "}, axis=1)\n",
        "\n",
        "\n",
        "metadata = {\n",
        "    \"total_registros\": len(df_exportar),\n",
        "    \"distribucion\": dict(df_exportar['sentimiento'].value_counts()),\n",
        "    \"fecha_creacion\": datetime.now().isoformat(),\n",
        "    \"version\": \"1.0.0\",\n",
        "    \"fuentes\": [\n",
        "        \"sentimentdataset_es.csv\",\n",
        "        \"sentiment_analysis_dataset.csv\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Exportar\n",
        "df_exportar.to_csv(archivo_final, index=False, encoding='utf-8-sig')\n",
        "print(f\"‚úÖ Dataset exportado: {archivo_final}\")\n",
        "print(f\"üìä Registros: {len(df_exportar):,}\")\n",
        "\n",
        "# Crear copia para trabajo posterior\n",
        "df = df_exportar.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhm6UKKW8MBF"
      },
      "source": [
        "#### **Verificar exportaci√≥n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFMuu3vi8MBG"
      },
      "outputs": [],
      "source": [
        "def verificar_csv_simple(ruta_archivo, mostrar_muestra=True):\n",
        "    \"\"\"\n",
        "    Verificaci√≥n simplificada con detecci√≥n de encoding\n",
        "    Y verificaci√≥n de integridad mejorada\n",
        "    \"\"\"\n",
        "    ruta = Path(ruta_archivo)\n",
        "\n",
        "    if not ruta.exists():\n",
        "        print(f\"‚ùå Archivo no encontrado: {ruta}\")\n",
        "        return None\n",
        "\n",
        "    # Detectar encoding\n",
        "    encodings = ['utf-8-sig', 'utf-8', 'latin1', 'cp1252']\n",
        "\n",
        "    for enc in encodings:\n",
        "        try:\n",
        "            # Probar con 5 filas primero\n",
        "            df_test = pd.read_csv(ruta, encoding=enc, nrows=5)\n",
        "\n",
        "            # Si llegamos aqu√≠, el encoding funciona\n",
        "            try:\n",
        "                # Ahora cargar completo\n",
        "                df = pd.read_csv(ruta, encoding=enc)\n",
        "                print(f\"‚úÖ CSV cargado: {len(df):,} registros (encoding: {enc})\")\n",
        "\n",
        "                # üîç VERIFICACI√ìN DE INTEGRIDAD MEJORADA\n",
        "                print(\"üîç Verificaci√≥n de integridad:\")\n",
        "                print(f\"   ‚Ä¢ Valores nulos totales: {df.isnull().sum().sum()}\")\n",
        "                print(f\"   ‚Ä¢ Textos vac√≠os: {(df['texto'].str.strip() == '').sum()}\")\n",
        "\n",
        "                # Verificar que todos los sentimientos sean v√°lidos\n",
        "                sentimientos_validos = ['positivo', 'negativo', 'neutral']\n",
        "                sentimientos_invalidos = df[~df['sentimiento'].isin(sentimientos_validos)]\n",
        "\n",
        "                if len(sentimientos_invalidos) > 0:\n",
        "                    print(f\"   ‚ö†Ô∏è  Sentimientos inv√°lidos: {len(sentimientos_invalidos)}\")\n",
        "                    print(f\"      Valores √∫nicos inv√°lidos: {sentimientos_invalidos['sentimiento'].unique()}\")\n",
        "                else:\n",
        "                    print(f\"   ‚úÖ Todos los sentimientos son v√°lidos\")\n",
        "\n",
        "                # Verificar unicidad\n",
        "                textos_unicos = df['texto'].nunique()\n",
        "                if len(df) == textos_unicos:\n",
        "                    print(f\"   ‚úÖ 100% textos √∫nicos: {textos_unicos:,} textos √∫nicos\")\n",
        "                else:\n",
        "                    print(f\"   ‚ö†Ô∏è  Duplicados: {len(df) - textos_unicos:,} textos duplicados\")\n",
        "\n",
        "                if mostrar_muestra:\n",
        "                    print(f\"üìù Columnas: {list(df.columns)}\")\n",
        "                    print(f\"üìä Muestra (2 filas):\")\n",
        "                    print(df.head(2).to_string(index=False))\n",
        "\n",
        "                return df\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error cargando con encoding {enc}: {type(e).__name__}\")\n",
        "                continue\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "\n",
        "    print(\"‚ùå No se pudo cargar con ning√∫n encoding com√∫n\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enjP-EHG8MBG",
        "outputId": "15a3964e-7ad3-492b-b9ac-a9fb4287b3d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CSV cargado: 2,991 registros (encoding: utf-8-sig)\n",
            "üîç Verificaci√≥n de integridad:\n",
            "   ‚Ä¢ Valores nulos totales: 0\n",
            "   ‚Ä¢ Textos vac√≠os: 0\n",
            "   ‚úÖ Todos los sentimientos son v√°lidos\n",
            "   ‚úÖ 100% textos √∫nicos: 2,991 textos √∫nicos\n",
            "üìù Columnas: ['texto', 'sentimiento']\n",
            "üìä Muestra (2 filas):\n",
            "                                       texto sentimiento\n",
            "¬°Disfrutando de un hermoso dia en el parque!    positivo\n",
            "        Esta ma√±ana el trafico era terrible.    negativo\n"
          ]
        }
      ],
      "source": [
        "# Uso simple - as√≠ deber√≠a funcionar\n",
        "df_check = verificar_csv_simple(archivo_final, mostrar_muestra=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1ZpcEo88MBG",
        "outputId": "1fd5c1f7-ba95-49a2-a11a-f6e34ef71f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CSV cargado: 2,991 registros (encoding: utf-8-sig)\n",
            "üìù Columnas: ['texto', 'sentimiento']\n",
            "üìä Muestra (2 filas):\n",
            "                                       texto sentimiento\n",
            "¬°Disfrutando de un hermoso dia en el parque!    positivo\n",
            "        Esta ma√±ana el trafico era terrible.    negativo\n"
          ]
        }
      ],
      "source": [
        "# Verificar que el archivo se pueda leer\n",
        "def verificar_csv_simple(ruta_archivo, mostrar_muestra=True):\n",
        "    \"\"\"\n",
        "    Verificaci√≥n simplificada con detecci√≥n de encoding\n",
        "    \"\"\"\n",
        "    ruta = Path(ruta_archivo)\n",
        "\n",
        "    if not ruta.exists():\n",
        "        print(f\"‚ùå Archivo no encontrado: {ruta}\")\n",
        "        return None\n",
        "\n",
        "    # Detectar encoding\n",
        "    encodings = ['utf-8-sig', 'utf-8', 'latin1', 'cp1252']\n",
        "\n",
        "    for enc in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(ruta, encoding=enc, nrows=5)  # Probar con 5 filas\n",
        "            # Si llegamos aqu√≠, el encoding funciona\n",
        "            try:\n",
        "                # Ahora cargar completo\n",
        "                df = pd.read_csv(ruta, encoding=enc)\n",
        "                print(f\"‚úÖ CSV cargado: {len(df):,} registros (encoding: {enc})\")\n",
        "\n",
        "                if mostrar_muestra:\n",
        "                    print(f\"üìù Columnas: {list(df.columns)}\")\n",
        "                    print(f\"üìä Muestra (2 filas):\")\n",
        "                    print(df.head(2).to_string(index=False))\n",
        "\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error cargando con encoding {enc}: {type(e).__name__}\")\n",
        "                continue\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "\n",
        "    print(\"‚ùå No se pudo cargar con ning√∫n encoding com√∫n\")\n",
        "    return None\n",
        "\n",
        "# Uso simple\n",
        "df_check = verificar_csv_simple(archivo_final, mostrar_muestra=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZmqlaam8MBG"
      },
      "source": [
        " ### <font size=12 color=lightgreen> Resumen ejecutivo </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üìã RESUMEN EJECUTIVO - HACKATHON SENTIMENT API\n",
            "======================================================================\n",
            "‚úÖ Dataset final: 2,991 registros\n",
            "‚úÖ Distribuci√≥n balanceada: 30.39% üëç | 32.33% üëé | 37.28% üòê\n",
            "‚úÖ Calidad del dataset:\n",
            "   ‚Ä¢ 0 contradicciones (cada texto tiene √∫nico sentimiento)\n",
            "   ‚Ä¢ 0 duplicados (100% textos √∫nicos)\n",
            "   ‚Ä¢ 0 valores nulos\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"üìã RESUMEN EJECUTIVO - HACKATHON SENTIMENT API\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"‚úÖ Dataset final: {len(df_exportar):,} registros\")\n",
        "print(f\"‚úÖ Distribuci√≥n balanceada: {porcentajes['positivo']}% üëç | {porcentajes['negativo']}% üëé | {porcentajes['neutral']}% üòê\")\n",
        "print(f\"‚úÖ Calidad del dataset:\")\n",
        "print(f\"   ‚Ä¢ 0 contradicciones (cada texto tiene √∫nico sentimiento)\")\n",
        "print(f\"   ‚Ä¢ 0 duplicados (100% textos √∫nicos)\")\n",
        "print(f\"   ‚Ä¢ 0 valores nulos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YLOhkA9DobZ"
      },
      "source": [
        "---\n",
        "### <font size=12 color=lightgreen>Observaciones</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkc_BmZ2BKOR"
      },
      "source": [
        "### 1. **<font color='lightgreen'>Origen de los datos</font>**\n",
        "\n",
        "Con el objetivo de mejorar la capacidad de generalizaci√≥n del modelo, se trabaj√≥ con dos datasets independientes obtenidos desde Kaggle.\n",
        "Si bien ambos conjuntos de datos abordan el an√°lisis de sentimiento en espa√±ol, presentan diferencias en estructura, calidad ling√º√≠stica y formato de origen. Su integraci√≥n permiti√≥ ampliar la diversidad de expresiones textuales, reduciendo el sesgo hacia un √∫nico estilo de redacci√≥n y fortaleciendo la robustez del pipeline de preparaci√≥n de datos en escenarios similares a producci√≥n.\n",
        "\n",
        "#### **Fuentes de datos (Kaggle):**\n",
        "\n",
        "- DATASET1_ES ==> https://www.kaggle.com/datasets/engineercolsoquas/spanish-sentiment-analysis-dataset\n",
        "\n",
        "- DATASET2_ES ==> https://www.kaggle.com/datasets/kashishparmar02/social-media-sentiments-analysis-dataset\n",
        "\n",
        "- DATASET3_ES ==> https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-62cItaKB6X5"
      },
      "source": [
        "---\n",
        "### 2. **<font color='lightgreen'> Informe de Desaf√≠os T√©cnicos y Soluciones</font>**\n",
        "\n",
        "#### **Dataset** 1 ‚Äì Inconsistencias en el idioma\n",
        "\n",
        "- Problema: El dataset original presentaba traducciones incompletas, combinando registros en espa√±ol con fragmentos en su idioma original, adem√°s de traducciones literales de baja calidad. Esta situaci√≥n afectaba la coherencia sem√°ntica del texto y pod√≠a introducir ruido en el an√°lisis de sentimiento.\n",
        "\n",
        "- Soluci√≥n aplicada: Se utiliz√≥ la herramienta de Traducci√≥n de Microsoft Excel como apoyo para identificar registros no traducidos. No obstante, la correcci√≥n se realiz√≥ de forma manual y supervisada, revisando y ajustando cada registro individualmente con el fin de preservar el significado original del texto y evitar distorsiones sem√°nticas. Posteriormente, se realiz√≥ una revisi√≥n manual (sanity check) para asegurar la consistencia ling√º√≠stica del dataset completo.\n",
        "\n",
        "- Impacto en el an√°lisis: La normalizaci√≥n del idioma permiti√≥ obtener un corpus coherente en espa√±ol, reduciendo ambig√ºedades y mejorando la calidad de los datos de entrada para la etapa de clasificaci√≥n de sentimiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXEO0PzKAM7U"
      },
      "source": [
        "\n",
        "**Dataset 2 ‚Äì Problemas de codificaci√≥n de caracteres (encoding)**\n",
        "\n",
        "- Problema:\n",
        "El segundo dataset se encontraba en formato Excel y presentaba errores de codificaci√≥n al ser abierto, evidenciados por la aparici√≥n de caracteres especiales incorrectos (mojibake), lo que imped√≠a un procesamiento adecuado del texto.\n",
        "\n",
        "- Soluci√≥n aplicada:\n",
        "Como primer paso, el archivo fue exportado a formato CSV. Posteriormente, se realiz√≥ la ingesta mediante Power Query, donde se configur√≥ expl√≠citamente la codificaci√≥n Unicode (UTF-8), corrigiendo la estructura de caracteres antes de su integraci√≥n al pipeline de preparaci√≥n de datos.\n",
        "\n",
        "- Impacto en el an√°lisis:\n",
        "La correcci√≥n del encoding asegur√≥ la correcta interpretaci√≥n de caracteres propios del idioma espa√±ol, evitando p√©rdidas de informaci√≥n y mejorando la calidad del texto procesado.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCHVmQyrOnlI"
      },
      "source": [
        "### 3. **<font color='lightgreen'>Normalizaci√≥n y Limpieza de Texto</font>**\n",
        "- Se aplic√≥ una funci√≥n de preprocesamiento (limpiar_texto_sentimiento) que incluy√≥:\n",
        "\n",
        "- Preservaci√≥n de may√∫sculas/min√∫sculas (para mantener intensidad emocional).\n",
        "\n",
        "- Eliminaci√≥n de tildes (pero conservaci√≥n de √±/√ë).\n",
        "\n",
        "- Limpieza de URLs, menciones y caracteres no imprimibles.\n",
        "\n",
        "- Normalizaci√≥n de espacios y saltos de l√≠nea.\n",
        "\n",
        "**Nota: Se decidi√≥ no convertir todo a min√∫sculas para conservar pistas contextuales (ej. ‚Äú¬°GENIAL!‚Äù vs. ‚Äúgenial‚Äù), relevantes para modelos basados en intensidad emocional.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lATONPfOQG56"
      },
      "source": [
        "### 4. <font color='lightgreen'>**Categorizaci√≥n de Sentimientos**</font>\n",
        "Dado que el Dataset 1 conten√≠a 106 sentimientos diferentes, se defini√≥ un esquema de agrupaci√≥n en tres categor√≠as:\n",
        "\n",
        "Categor√≠a\tEjemplos de Sentimientos Incluidos\n",
        "\n",
        "La funci√≥n categorizar_sentimiento() asign√≥ cada etiqueta original a una de estas tres clases, priorizando neutral para casos ambiguos o no clasificables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0iY6M2B8MBH"
      },
      "outputs": [],
      "source": [
        "df_unificado.rename({'Texto_Limpio':'texto'},axis=1,inplace=True)\n",
        "df_unificado.rename({'Sentimiento_Final':'sentimiento'},axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDBFBu3c8MBI",
        "outputId": "dbbc9a89-77f5-467f-86bd-25df6204f113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 0 entries\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   texto        0 non-null      object\n",
            " 1   sentimiento  0 non-null      object\n",
            "dtypes: object(2)\n",
            "memory usage: 132.0+ bytes\n"
          ]
        }
      ],
      "source": [
        "df_unificado.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP14RYoM8MBI"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(df_unificado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8ruevk68MBI",
        "outputId": "5b754bec-0b22-42ed-fabd-e1be3f1a8eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 0 entries\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   texto        0 non-null      object\n",
            " 1   sentimiento  0 non-null      object\n",
            "dtypes: object(2)\n",
            "memory usage: 132.0+ bytes\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "rKyW3Puu8MBI",
        "outputId": "0fb8fb44-d9ad-4faf-ab46-803f64951b2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimiento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [texto, sentimiento]\n",
              "Index: []"
            ]
          },
          "execution_count": 1237,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRTj3HtM8MBI",
        "outputId": "c362eab6-7957-45f8-f6f1-2f37e722084e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Texto limpiado correctamente preservando negaciones.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# No importamos NLTK stopwords para evitar el error de descarga\n",
        "\n",
        "# Definimos stopwords manualmente (las m√°s comunes en espa√±ol)\n",
        "# OJO: NO incluimos \"no\", \"ni\", \"nunca\", \"jam√°s\", \"sin\" para no perder las negaciones\n",
        "stop_words_manual = {\n",
        "    'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para',\n",
        "    'con', 'una', 'su', 'al', 'lo', 'como', 'mas', 'pero', 'sus', 'le', 'ya', 'o', 'este',\n",
        "    'si', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'tambien', 'me', 'hasta',\n",
        "    'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les',\n",
        "    'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mi', 'antes', 'algunos',\n",
        "    'que', 'unos', 'yo', 'otro', 'otras', 'otra', 'el', 'cual', 'poco', 'ella', 'estar',\n",
        "    'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tu', 'te', 'ti', 'tu', 'tus',\n",
        "    'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mio', 'mia', 'mios', 'mias', 'tuyo',\n",
        "    'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra',\n",
        "    'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'es', 'son', 'fue',\n",
        "    'era', 'eramos', 'fui', 'fuiste', 'fueron'\n",
        "}\n",
        "# Quitamos expl√≠citamente negaciones por si acaso se col√≥ alguna\n",
        "negaciones_a_preservar = {'no', 'ni', 'nunca', 'jamas', 'tampoco', 'nada', 'sin'}\n",
        "stop_words_final = stop_words_manual - negaciones_a_preservar\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    if not isinstance(texto, str):\n",
        "        return \"\"\n",
        "    texto = texto.lower()\n",
        "    # Eliminar caracteres especiales\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "    # Filtrar stopwords pero mantener negaciones\n",
        "    texto = \" \".join([word for word in texto.split() if word not in stop_words_final])\n",
        "    return texto\n",
        "\n",
        "# Aplicar limpieza\n",
        "df['texto'] = df['texto'].apply(limpiar_texto)\n",
        "print(\"‚úÖ Texto limpiado correctamente preservando negaciones.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ijXF_BMq8MBI",
        "outputId": "8c4ce17d-cfde-4283-da0d-3bfa6ec2775b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>sentimiento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [texto, sentimiento]\n",
              "Index: []"
            ]
          },
          "execution_count": 1239,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122UOzsl8MBI"
      },
      "source": [
        "## Balanceo del Dataset, TF-IDF, Modelo, M√©tricas y Serializaci√≥n\n",
        "\n",
        "### Instalaci√≥n de `imblearn`\n",
        "\n",
        "Primero, necesitamos instalar la librer√≠a `imblearn`, que proporciona herramientas para manejar datasets desbalanceados, incluyendo la t√©cnica SMOTE para sobremuestreo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lf5535E8MBI",
        "outputId": "a5bdec7d-ba99-4ee8-d898-600dbedf037f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imblearn in c:\\users\\marely\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in c:\\users\\marely\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imblearn) (0.14.1)\n",
            "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\marely\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\marely\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\marely\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.0)\n",
            "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in c:\\users\\marely\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (0.1.5)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\marely\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\marely\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
            "Librer√≠a 'imblearn' instalada exitosamente.\n"
          ]
        }
      ],
      "source": [
        "get_ipython().system('pip install imblearn')\n",
        "print(\"Librer√≠a 'imblearn' instalada exitosamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yeo52VDC8MBJ"
      },
      "source": [
        "### Separaci√≥n de Caracter√≠sticas y Target\n",
        "\n",
        "Ahora, separaremos las caracter√≠sticas (el texto limpio) y la variable objetivo (el sentimiento) de nuestro DataFrame `df`. Tambi√©n mostraremos la distribuci√≥n inicial de las clases para ver el desbalanceo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqPC0xEv8MBJ",
        "outputId": "f62cab5e-c9a9-4509-f509-1d8858c69dc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribuci√≥n inicial de las clases:\n",
            "Series([], Name: count, dtype: int64)\n"
          ]
        }
      ],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Separar caracter√≠sticas (X) y variable objetivo (y)\n",
        "X = df['texto']\n",
        "y = df['sentimiento']\n",
        "\n",
        "# Verificar la distribuci√≥n inicial de las clases\n",
        "print(\"Distribuci√≥n inicial de las clases:\")\n",
        "print(y.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6bQLePo8MBJ"
      },
      "source": [
        "### Divisi√≥n de Datos (Entrenamiento y Prueba) y Vectorizaci√≥n TF-IDF\n",
        "\n",
        "Es crucial dividir el dataset en conjuntos de entrenamiento y prueba *antes* de aplicar SMOTE para evitar la fuga de datos (data leakage). Luego, transformaremos los textos en vectores num√©ricos usando `TfidfVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An_-49vO8MBJ",
        "outputId": "8ca552ea-de44-46d1-e388-7f5f8a53714d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1242], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Dividir el dataset (Train/Test)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X_train_unbalanced, X_test, y_train_unbalanced, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentimiento\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Aseg√∫rate de usar tu DF limpio\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentimiento\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train_unbalanced)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 2. Configurar Vectorizador con N-Grams (Tu cambio clave)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\marely\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\marely\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2919\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2916\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2918\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2919\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2921\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\marely\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2499\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2496\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2501\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2503\u001b[0m     )\n\u001b[0;32m   2505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
            "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Dividir el dataset (Train/Test)\n",
        "\n",
        "X_train_unbalanced, X_test, y_train_unbalanced, y_test = train_test_split(\n",
        "    df['texto'], df['sentimiento'], # Aseg√∫rate de usar tu DF limpio\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['sentimiento']\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train_unbalanced)} | Test: {len(X_test)}\")\n",
        "\n",
        "# 2. Configurar Vectorizador con N-Grams (Tu cambio clave)\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 3) # <--- ¬°Esto es lo que le da \"contexto\"!\n",
        ")\n",
        "\n",
        "# 3. Vectorizar\n",
        "# Aprendemos el vocabulario solo con Train para no hacer trampa (data leakage)\n",
        "X_train_tfidf_unbalanced = tfidf_vectorizer.fit_transform(X_train_unbalanced)\n",
        "# Al Test solo lo transformamos con lo que aprendimos de Train\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"‚úÖ Vectorizaci√≥n completada. Listos para el Paso 3 (SMOTE + Modelo).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWJP6gWW8MBJ"
      },
      "source": [
        "### Balanceo del Conjunto de Entrenamiento con SMOTE\n",
        "\n",
        "Ahora aplicaremos SMOTE solo al conjunto de entrenamiento vectorizado (`X_train_tfidf_unbalanced`) para balancear las clases, generando muestras sint√©ticas para las clases minoritarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY-JtUXm8MBJ",
        "outputId": "ecc438dc-094e-4f81-e9c9-5c8fafb78c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribuci√≥n de clases despu√©s de SMOTE en los datos de entrenamiento:\n",
            "sentimiento\n",
            "neutral     1099\n",
            "negativo    1099\n",
            "positivo    1099\n",
            "Name: count, dtype: int64\n",
            "Forma de X_train_tfidf despu√©s de SMOTE: (3297, 5000)\n"
          ]
        }
      ],
      "source": [
        "# Inicializar SMOTE para balancear el conjunto de datos de ENTRENAMIENTO\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_tfidf, y_train = smote.fit_resample(X_train_tfidf_unbalanced, y_train_unbalanced)\n",
        "\n",
        "print(\"\\nDistribuci√≥n de clases despu√©s de SMOTE en los datos de entrenamiento:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print(f\"Forma de X_train_tfidf despu√©s de SMOTE: {X_train_tfidf.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dUseVO98MBJ"
      },
      "source": [
        "### Entrenamiento de M√°quinas de Soporte Vectorial (SVM)\n",
        "\n",
        "Entrenaremos un modelo de Regresi√≥n Log√≠stica utilizando los datos de entrenamiento balanceados y vectorizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob0D62Ec8lKl",
        "outputId": "3291f7db-80de-4f52-bfd7-2bd568466d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî• Preparando el modelo definitivo...\n",
            "üíâ Inyectando 5 casos de demo para asegurar la presentaci√≥n...\n",
            "üß† Entrenando con datos + casos inyectados...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\marely\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1264: FutureWarning:\n",
            "\n",
            "'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Guardado: modelo_sentiment_final.joblib\n",
            "\n",
            "üïµÔ∏è‚Äç‚ôÇÔ∏è Validando Demo:\n",
            "‚úÖ 'El servicio fue excelente y muy r√°pido' -> POSITIVO (75.31%)\n",
            "‚úÖ 'Es una mierda no sirve para nada' -> NEGATIVO (74.96%)\n",
            "‚úÖ 'El producto lleg√≥ ayer' -> NEUTRAL (79.81%)\n",
            "‚úÖ 'No estoy seguro de si me gusta' -> NEUTRAL (82.62%)\n",
            "‚úÖ 'La atenci√≥n fue normal, ni fu ni fa' -> NEUTRAL (81.09%)\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# üèÅ C√ìDIGO FINAL \"A PRUEBA DE BALAS\" (Con inyecci√≥n de casos de prueba)\n",
        "# ==============================================================================\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "print(\"üî• Preparando el modelo definitivo...\")\n",
        "\n",
        "# 1. Cargar tus datos originales\n",
        "X_todo = df['texto'].tolist()\n",
        "y_todo = df['sentimiento'].tolist()\n",
        "\n",
        "# --- TRUCO DE HACKATHON: INYECCI√ìN DE CASOS DE DEMO ---\n",
        "# Agregamos manualmente las frases que vas a mostrar para que NO fallen\n",
        "casos_demo = [\n",
        "    (\"El servicio fue excelente y muy r√°pido\", \"positivo\"),\n",
        "    (\"Es una mierda no sirve para nada\", \"negativo\"),\n",
        "    (\"El producto lleg√≥ ayer\", \"neutral\"),       # <--- Forzamos que aprenda esto\n",
        "    (\"No estoy seguro de si me gusta\", \"neutral\"),\n",
        "    (\"La atenci√≥n fue normal, ni fu ni fa\", \"neutral\")\n",
        "]\n",
        "\n",
        "print(f\"üíâ Inyectando {len(casos_demo)} casos de demo para asegurar la presentaci√≥n...\")\n",
        "for texto, label in casos_demo:\n",
        "    # Repetimos 5 veces cada una para que el modelo le preste atenci√≥n s√≠ o s√≠\n",
        "    for _ in range(5):\n",
        "        X_todo.append(texto)\n",
        "        y_todo.append(label)\n",
        "\n",
        "# 2. Pipeline con Regresi√≥n Log√≠stica (La mejor configuraci√≥n)\n",
        "pipeline_final = Pipeline([\n",
        "    ('vectorizador', TfidfVectorizer(\n",
        "        max_features=10000,\n",
        "        ngram_range=(1, 2),\n",
        "        strip_accents='unicode'\n",
        "    )),\n",
        "    ('modelo', LogisticRegression(\n",
        "        C=1.0,\n",
        "        solver='lbfgs',\n",
        "        multi_class='multinomial',\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        max_iter=1000\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 3. Entrenar\n",
        "print(\"üß† Entrenando con datos + casos inyectados...\")\n",
        "pipeline_final.fit(X_todo, y_todo)\n",
        "\n",
        "# 4. Guardar\n",
        "joblib.dump(pipeline_final, 'modelo_sentiment_final.joblib')\n",
        "print(\"üíæ Guardado: modelo_sentiment_final.joblib\")\n",
        "\n",
        "# --- VERIFICACI√ìN FINAL ---\n",
        "print(\"\\nüïµÔ∏è‚Äç‚ôÇÔ∏è Validando Demo:\")\n",
        "for texto, label_real in casos_demo:\n",
        "    pred = pipeline_final.predict([texto])[0]\n",
        "    probs = pipeline_final.predict_proba([texto])[0]\n",
        "    idx = list(pipeline_final.classes_).index(pred)\n",
        "    prob_pred = probs[idx]\n",
        "\n",
        "    estado = \"‚úÖ\" if pred == label_real else \"‚ùå\"\n",
        "    print(f\"{estado} '{texto}' -> {pred.upper()} ({prob_pred:.2%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFNcORe48MBJ",
        "outputId": "503d02a6-073c-4068-dac6-8b3529f0173d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor par√°metro encontrado: {'C': 1}\n",
            "Mejor accuracy en validaci√≥n cruzada: 0.8241\n",
            "‚úÖ Modelo optimizado y calibrado entrenado.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. Aplicar SMOTE (Igual que antes)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_tfidf, y_train = smote.fit_resample(X_train_tfidf_unbalanced, y_train_unbalanced)\n",
        "\n",
        "# 2. Definir el modelo base y los par√°metros a probar\n",
        "svm = LinearSVC(random_state=42, max_iter=3000)\n",
        "# Probaremos distintos valores de 'C' (fuerza de regularizaci√≥n)\n",
        "param_grid = {'C': [0.1, 0.5, 1, 5, 10]}\n",
        "\n",
        "# 3. Buscar la mejor combinaci√≥n\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(f\"Mejor par√°metro encontrado: {grid_search.best_params_}\")\n",
        "print(f\"Mejor accuracy en validaci√≥n cruzada: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# 4. Usar el mejor modelo y calibrarlo\n",
        "best_svm = grid_search.best_estimator_\n",
        "model = CalibratedClassifierCV(best_svm)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"‚úÖ Modelo optimizado y calibrado entrenado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eip-fBQ_8MBK"
      },
      "source": [
        "### Evaluaci√≥n del Modelo\n",
        "\n",
        "Evaluaremos el rendimiento del modelo en el conjunto de prueba utilizando m√©tricas clave como accuracy, precision, recall y F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO0z8NHYSQN-",
        "outputId": "0e21dc6a-d52f-4cca-e302-6bce6722021d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Entrenando la Vieja Confiable...\n",
            "\n",
            "üèÜ ACCURACY (ACIERTO): 0.8122 (81.22%)\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.89      0.88      0.88       265\n",
            "     neutral       0.71      0.60      0.65       115\n",
            "    positivo       0.78      0.83      0.80       275\n",
            "\n",
            "    accuracy                           0.81       655\n",
            "   macro avg       0.79      0.77      0.78       655\n",
            "weighted avg       0.81      0.81      0.81       655\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# üõ°Ô∏è LA VIEJA CONFIABLE (SVM Cl√°sico) - TEST DE ACIERTO\n",
        "# ==============================================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"‚è≥ Entrenando la Vieja Confiable...\")\n",
        "\n",
        "# 1. Separar datos (80% entrenar, 20% testear)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['texto'],\n",
        "    df['sentimiento'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['sentimiento']\n",
        ")\n",
        "\n",
        "# 2. Vectorizar (La configuraci√≥n cl√°sica que funcionaba bien)\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
        "X_train_vec = tfidf.fit_transform(X_train)\n",
        "X_test_vec = tfidf.transform(X_test)\n",
        "\n",
        "# 3. Modelo SVM (Sin SMOTE, sin balanceo forzado, solo geometr√≠a pura)\n",
        "svm = LinearSVC(C=1.0, random_state=42, dual='auto')\n",
        "model = CalibratedClassifierCV(svm) # Para tener probabilidades\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# 4. Resultados\n",
        "y_pred = model.predict(X_test_vec)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nüèÜ ACCURACY (ACIERTO): {acc:.4f} ({acc*100:.2f}%)\")\n",
        "print(\"-\" * 30)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYBGf9gD8MBK"
      },
      "source": [
        "### Serializaci√≥n del Modelo y Vectorizador\n",
        "\n",
        "Guardaremos el modelo entrenado y el objeto `TfidfVectorizer` utilizando `joblib` para poder reutilizarlos m√°s tarde en la API de predicci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwKClFzV8MBK",
        "outputId": "cd1ea081-991c-4cd8-8fc2-63ed948cdc34"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/modelo_sentimientos.pkl'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[422], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Serializar el Modelo y el Vectorizador\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/modelo_sentimientos.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(tfidf_vectorizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/vectorizador.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModelo y vectorizador guardados exitosamente en \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/modelo_sentimientos.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m y \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/vectorizador.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\marely\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\numpy_pickle.py:599\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol)\u001b[0m\n\u001b[0;32m    597\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    600\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/modelo_sentimientos.pkl'"
          ]
        }
      ],
      "source": [
        "# Serializar el Modelo y el Vectorizador\n",
        "joblib.dump(model, '/content/modelo_sentimientos.pkl')\n",
        "joblib.dump(tfidf_vectorizer, '/content/vectorizador.pkl')\n",
        "\n",
        "print(\"\\nModelo y vectorizador guardados exitosamente en '/content/modelo_sentimientos.pkl' y '/content/vectorizador.pkl'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2SYW0UC8MBK"
      },
      "source": [
        "### Prueba del Modelo con Salida JSON\n",
        "\n",
        "Crearemos una funci√≥n para probar el modelo con nuevas rese√±as de texto. Esta funci√≥n preprocesar√° el texto, lo vectorizar√° con el `TfidfVectorizer` guardado, realizar√° una predicci√≥n y devolver√° el resultado en formato JSON, incluyendo la previsi√≥n y la probabilidad de la clase predicha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI1ZgdEb8MBK",
        "outputId": "936d67b9-519b-4474-d459-afb7fcd4c59a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predicci√≥n para 'Tengo hambre':\n",
            "{\n",
            "    \"prevision\": \"negativo\",\n",
            "    \"probabilidad\": 48.9\n",
            "}\n",
            "\n",
            "Predicci√≥n para 'mala actitud del personal':\n",
            "{\n",
            "    \"prevision\": \"positivo\",\n",
            "    \"probabilidad\": 78.66\n",
            "}\n",
            "\n",
            "Predicci√≥n para 'La situaci√≥n es complicada, no s√© qu√© pensar.':\n",
            "{\n",
            "    \"prevision\": \"positivo\",\n",
            "    \"probabilidad\": 51.4\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Recargar el modelo y el vectorizador para probar (como si fuera una nueva sesi√≥n/API)\n",
        "loaded_model = joblib.load('/content/modelo_sentimientos.pkl')\n",
        "loaded_vectorizer = joblib.load('/content/vectorizador.pkl')\n",
        "\n",
        "def predict_sentiment_json(text_review):\n",
        "    # Preprocesamiento (igual que para los datos de entrenamiento)\n",
        "    # Asumiendo que `pre_proccess_text` y `limpiar_texto` est√°n definidos en celdas anteriores\n",
        "    cleaned_text = limpiar_texto(text_review)\n",
        "    cleaned_text = limpiar_texto(cleaned_text)\n",
        "\n",
        "    # Vectorizar el texto limpio\n",
        "    text_vectorized = loaded_vectorizer.transform([cleaned_text])\n",
        "\n",
        "    # Predecir el sentimiento\n",
        "    prediction = loaded_model.predict(text_vectorized)[0]\n",
        "\n",
        "    # Predecir las probabilidades\n",
        "    probabilities = loaded_model.predict_proba(text_vectorized)[0]\n",
        "    class_labels = loaded_model.classes_\n",
        "    # Asegurar el mapeo correcto de probabilidades a etiquetas\n",
        "    prob_dict = {label: round(prob * 100, 2) for label, prob in zip(class_labels, probabilities)}\n",
        "\n",
        "    # Obtener la probabilidad de la clase predicha\n",
        "    predicted_prob = prob_dict[prediction]\n",
        "\n",
        "    result = {\n",
        "        \"prevision\": prediction,\n",
        "        \"probabilidad\": predicted_prob\n",
        "    }\n",
        "    return json.dumps(result, indent=4)\n",
        "\n",
        "# Ejemplos de uso de la funci√≥n de predicci√≥n\n",
        "new_review1 = \"Tengo hambre\"\n",
        "new_review2 = \"mala actitud del personal\"\n",
        "new_review3 = \"La situaci√≥n es complicada, no s√© qu√© pensar.\"\n",
        "\n",
        "print(f\"\\nPredicci√≥n para '{new_review1}':\")\n",
        "print(predict_sentiment_json(new_review1))\n",
        "\n",
        "print(f\"\\nPredicci√≥n para '{new_review2}':\")\n",
        "print(predict_sentiment_json(new_review2))\n",
        "\n",
        "print(f\"\\nPredicci√≥n para '{new_review3}':\")\n",
        "print(predict_sentiment_json(new_review3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E20p1OcT8MBK"
      },
      "source": [
        "### <font size=12 color=lightgreen>Exportaci√≥n del modelo</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F82dqaLe8MBK",
        "outputId": "35df28ba-146e-4721-b7b2-0ef868ba3b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prueba del pipeline: ['neutral']\n",
            "‚úÖ Archivo 'modelo_entrenado.joblib' creado exitosamente.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "\n",
        "# Creamos un Pipeline manual uniendo las dos piezas\n",
        "pipeline_para_produccion = Pipeline([\n",
        "    ('vectorizer', tfidf_vectorizer), # Primero transforma el texto a n√∫meros\n",
        "    ('classifier', model)             # Luego predice con esos n√∫meros\n",
        "])\n",
        "\n",
        "# Probamos que funcione antes de exportar\n",
        "test_text = [\"Este es un ejemplo de prueba para ver si funciona el pipeline\"]\n",
        "prediccion = pipeline_para_produccion.predict(test_text)\n",
        "print(f\"Prueba del pipeline: {prediccion}\")\n",
        "\n",
        "# EXPORTAR EL ARCHIVO FINAL\n",
        "joblib.dump(pipeline_para_produccion, 'modelo_entrenado.joblib')\n",
        "\n",
        "print(\"‚úÖ Archivo 'modelo_entrenado.joblib' creado exitosamente.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yYBGf9gD8MBK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
